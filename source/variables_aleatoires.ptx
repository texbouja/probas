<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch-varalea" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Variables aléatoires discrètes</title>

  <section xml:id="sec-varalea">
    <title>Variables aléatoires</title>

    <p>
      On se donne dans ce chapitre un espace probabilisé <m>(\Omega,\mathscr T,\Pr)</m> et un espace probabilisable <m>(\Omega',\mathscr T')</m>
    </p>

    <definition xml:id="def-varalea">
      <statement>
        <p>
          On appelle variable aléatoire de <m>(\Omega,\mathscr T)</m> dans <m>(\Omega',\mathscr T')</m> toute application <m>X:\Omega\longrightarrow \Omega'</m> telle que
          <me>
            \forall A'\in\mathscr T',\; X^{-1}(A')\in\mathscr T
          </me>
          c'est à dire que l'image réciproque par <m>X</m> de tout événement est un événement.
        </p>

        <p>
          Soit dans la suite <m>X</m> une variable aléatoire de <m>(\Omega,\mathscr T)</m> dans <m>(\Omega',\mathscr T')</m>.
        </p>

        <p>
          <m>X</m> est dite discrète si l'ensemble <m>X(\Omega)</m> est au plus dénombrable et <m>\mathscr P\bigl(X(\Omega)\bigr)\subset \mathcal T'</m>. Ce qui équivaut à
          <me>
            \forall x\in X(\Omega),\; \{x\}\in \mathscr T'
          </me>
        </p>

        <p>
          X est dite réelle si <m>\Omega'\subset \R</m>.
        </p>
      </statement>
      <explanation>
      <p>
        Dans la pratique une variable aléatoire est utilisée pour représenter le résultat d'une expérience aléatoire.
        Souvent l'univers <m>\Omega</m> et la tribu <m>\mathscr T</m> ne sont pas précisés.
        Les résultats et les événements de l'expérience sont  dans <m>\Omega'</m> et dans <m>\mathscr T'</m>.
      </p>

      <p>
        La question est comment choisir <m>(\Omega',\mathscr T')</m> et de quelle tribu munir <m>\Omega</m> pour que le résultat de l'expérience soit une variable aléatoire ? Les remarques suivantes donnent des éléments de réponse.
      </p>
      </explanation>
    </definition>

    <remark>
      <ol>
        <li>
          <p>
            Sauf précision du contraire, un univers <m>\Omega</m> au plus dénombrable sera systématiquement muni de la tribu <m>\mathscr P(\Omega)</m>.
          </p>
        </li>

        <li>
          <p>
            Si <m>\Omega</m> est au plus dénombrable ( et muni de la tribu <m>\mathscr P(\Omega)</m> ) alors toute application définie sur <m>\Omega</m> est une variable aléatoire quelque soit la tribu considérée dans l'espace d'arrivée.
          </p>
        </li>

        <li>
          <p>
            Soit <m>f</m> une application quelconque définie de <m>\Omega</m> dans <m>\Omega'</m>.
            L'ensemble
            <me>
              \mathscr T_f=\bigl\{ f^{-1}(A')\mid A'\in\mathscr T'\}
            </me>
            est une tribu de <m>\Omega</m> et <m>f</m> est une variable aléatoire de <m>(\Omega,\mathscr T_f)</m> dans <m>(\Omega,\mathscr T)</m>.
          </p>

          <p>
            Si <m>\Omega'</m> est au plus dénombrable et <m>\mathscr T'=\mathscr P(\Omega')</m>, on voit qu'une application quelconque de <m>\Omega</m> dans <m>\Omega'</m> peut être considérée comme une variable aléatoire avec très peu de contraintes.
            Il suffit de se placer du côté de <m>\Omega</m> dans une tribu qui contient <m>\mathscr T_f</m>.
          </p>

          <p>
            Si <m>f_1,f_2,\ldots,f_p</m> sont des applications définies sur <m>\Omega</m> telles que <m>f_k(\Omega)</m> soit au plus dénombrable pour tout <m>k\in[\![1,n]\!]</m> alors on peut poser :
            <ul>
              <li>
                <m>\Omega'=\bigcup_{k=1}^p f_k(\Omega) \text{ et } \mathscr T'=\mathscr P(\Omega');</m>
              </li>

              <li>
                <m>\mathscr T=\sigma\biggl(\bigcup_{k=1}^p\mathscr T_{f_k}\biggr)</m>
              </li>
            </ul>
            de telle sorte que les applications <m>f_1,f_2,\ldots,f_p</m> soient toutes des variables aléatoires discrètes de  <m>(\Omega,\mathscr T)</m> dans <m>(\Omega',\mathscr T')</m>.
          </p>

          <p>
            Ses observations restent valides pour une famille dénombrable d'applications <m>(f_i)_{i\in I}</m> définies sur <m>\Omega</m> telle que <m>f_i(\Omega)</m> soit au plus dénombrable pour tout <m>i\in I</m>.
            Notamment pour une suite de telles applications.
          </p>

          <p>
            C'est ainsi qu'il est toujours possible de considérer un modèle dans lequel on peut combiner entre les résultats d'un nombre fini ou dénombrable d'expériences aléatoires si chacune a au plus un ensemble au plus dénombrable de résultats.
          </p>
        </li>
      </ol>
    </remark>


    <proposition xml:id="prop-compvar">
      <statement>
        <ol>
          <li>
            <p>
              La composée <m>Y\circ X</m> de deux variables aléatoires <m>X</m> et <m>Y</m> est une variable aléatoire.
              De plus si <m>Y</m> est discrète alors <m>Y\circ X</m> est discrète.
            </p>
          </li>

          <li>
            <p>
              Si pour tout <m>k\in[\![1,p]\!]</m>, <m>X_k</m> est une variable aléatoire de <m>(\Omega,\mathscr T)</m> dans un espace probabilisable <m>(\Omega_k,\mathscr T_k)</m> alors l'application <m>(X_1,X_2,\ldots,X_p)</m> définie par
              <me>
                \forall \omega\in\Omega, (X_1,X_2,\ldots,X_p)(\omega)=(X_1(\omega),X_2(\omega),\ldots,X_p(\omega))
              </me>
              est une variable aléatoire de <m>(\Omega,\mathscr T)</m> dans <m>(\Omega_1\times\cdots\times\Omega_p,\mathscr T_1\times\cdots\times\mathscr T_p)</m>.
              De plus si <m>X_1,X_2,\ldots,X_p</m> sont discrètes alors <m>(X_1,X_2,\ldots,X_p)</m> est discrète.
            </p>
          </li>

          <li>
            <p>
              Soit maintenant une variable aléatoire discrète <m>X</m> de <m>(\Omega,\mathscr T)</m> dans <m>(\Omega',\mathscr T')</m>.
              Alors pour toute application <m>f</m> définie sur <m>X(\Omega)</m> l'application <m>f\circ X</m> est une VAD.
              On la note <m>f(X)</m>
              <me>
                \forall \omega\in\Omega,\; f(X)(\omega):=f(X(\omega))
              </me>
            </p>

            <p>
              On généralise de la façon suivante : si <m>X_1,X_2,\ldots,X_p</m> sont des VAD definies sur <m>(\Omega,\mathscr T)</m>  alors pour toute application <m>g</m> définie sur <m>X_1(\Omega)\times X_2(\Omega)\times\cdots\times X_p(\Omega)</m> on définit la variable aléatoire discrète <m>g(X_1,X_2,\ldots,X_p)</m> par
              <me>
                \forall \omega\in\Omega,\; g(X_1,X_2,\ldots,X_p)(\omega):=g(X_1(\omega),X_2(\omega),\ldots,X_p(\omega))
              </me>
            </p>
          </li>
        </ol>
      </statement>
    </proposition>

    <definition>
      <ol>
        <li>
          <p>
            étant donné des variables aléatoires discrètes <m>X, X_1,\ldots,X_n</m> de <m>(\Omega, \mathscr T)</m> dans <m>(\Omega', \mathscr T')</m>, on note
            <ul>
              <li>
                pour tout <m>x\in \mathscr T'</m>
                <me>
                  (X=x)=X^{-1}(\{x\})=\{\omega\in\Omega\mid X(\omega)=x\}
                </me>
              </li>

              <li>
                pour tous <m>x_1,x_2,\ldots,x_n\in \mathscr T'</m>
                <me>
                  (X_1=x_1,X_2=x_2,\ldots,X_n=x_n)=\bigcap_{k=1}^n(X_k=x_k)
                </me>
              </li>

              <li>
                pour tout  <m>A'\in \mathscr T'</m>,
                <me>
                  (X\in A')=X^{-1}(A')=\{\omega\in\Omega\mid X(\omega)\in A'\}
                </me>
              </li>

              <li>
                pour tous <m> A_1',A_2',\ldots,A_n'\in \mathscr T'</m>
                <me>
                  (X_1\in A'_1,X_2\in A_2',\ldots,X_n\in A'_n)=\bigcap_{k=1}^n(X_k\in A_k')
                </me>
              </li>
            </ul>
          </p>
        </li>

        <li>
          <p>
            Une variable aléatoire discrète <m>X</m> est dite presque partout constante s'il existe <m>c\in X(\Omega)</m> tel que <m>\Pr(X=c)=1</m>.
            Elle est en particulier dite presque partout nulle si <m>\Pr(X=0)=1</m>.
          </p>
        </li>
      </ol>
    </definition>

    <remark>
      <p>
        Vu la tolérance de l'image réciproque par une application envers les opérations sur les ensembles, les notations précédentes donnent
        <ul>
          <li>
            <p>
              <m>(X\in A')=\bigcup_{x\in A'}(X=x)</m>
            </p>
          </li>

          <li>
            <p>
              <m>(X\in\bigcup_{i\in I}A_i')=\bigcup_{i\in I}(X\in A_i')</m>
            </p>
          </li>

          <li>
            <p>
              <m>(X\in\bigcap_{i\in I}A_i')=\bigcap_{i\in I}(X\in A_i')</m>
            </p>
          </li>

          <li>
            <p>
              <m>(X\in A')^c=(X\in (A')^c)=(X\notin A')</m>
            </p>
          </li>
        </ul>
      </p>
    </remark>

    <example>
      <title>d'utilisation de ces notations</title>

      <ul>
        <li>
          <p>
            Si <m>X</m> et <m>Y</m> sont des VAD à valeurs dans <m>\N</m> alors pour tout <m>n\in N</m>,
            <me>
              (X+Y=n)=\bigcup_{k=0}^n(X=k,Y=n-k)
            </me>
            et puisque ces événements sont deux à deux disjoints alors
            <me>
              \Pr(X+Y=n)=\sum_{k=0}^n\Pr(X=k,Y=n-k)
            </me>
          </p>
        </li>

        <li>
          <p>
            Si <m>N</m> est une VAD à valeurs dans <m>\N\cup\{\infty\}</m> alors
            <me>
              (N=\infty)=(N\in\N)^c=\left(\bigcup_{n=0}^{+\infty}(N=n)\right)^c=\bigcap_{n=0}^{\infty}(N\ne n)
            </me>
            et particulier
            <me>
              \Pr(N=\infty)=1-\sum_{n=0}^{+\infty}\Pr(N=n)=\lim_{n\to+\infty}\Pr\biggl(\bigcap_{k=0}^n(N\ne k)\biggr)
            </me>
          </p>
        </li>

        <li>
          <p>
            <m>(f(X)=y)=\bigcup\limits_{x\in f^{-1}(y)}(X=x)</m>. Par exemple, si <m>y\in[-1,1]</m> alors <m>(\sin(X)=y)=\bigcup\limits_{k\in\Z}(X=\arcsin(x)+2k\pi)</m>
          </p>
        </li>
      </ul>
    </example>

    <theorem>
      <statement>
        <p>
          Soit <m>X</m> une variables aléatoire discrète de <m>(\Omega,\mathscr T)</m> dans <m>(\Omega',\mathscr T')</m>.
          <ol>
            <li>
              <p>
                <m>\bigl((X=x)\bigr)_{x\in X(\Omega)}</m> est un système complet d'événements de <m>(\Omega,\mathscr T)</m>;
              </p>
            </li>

            <li>
              <p>
                Pour tout <m>A'\in\mathscr T'</m>, <m>\displaystyle \Pr\bigl(X\in A')=\sum_{x\in A'}\Pr(X=x)</m>
              </p>
            </li>
          </ol>
          On appelle loi de la variable <m>X</m> le couple <m>\bigl(X,(\Pr(X=x))_{x\in X(\Omega)}\bigr)</m>.
          L'application <m>x\longmapsto P(X=x)</m> est dite fonction des masses de la variable <m>X</m>.
        </p>
        <explanation>
        <p>
          La loi d'une VADR est ainsi le couple formé de l'ensemble des valeurs qu'elle peut prendre et de la famille (au plus dénombrable) des probabilités pour que chacune de ces valeurs se réalise.
          La propriété 2 signifie que les probabilités des événements <m>(X\in A')</m> sont déterminées par la loi de <m>X</m>.
        </p>
        </explanation>
      </statement>
    </theorem>

    <corollary xml:id="cor-loiX">
      <statement>
        <p>
          Soit <m>X</m> une variable aléatoire discrète de <m>(\Omega,\mathscr T)</m> dans <m>(\Omega',\mathscr T')</m>.
          L'application
          <me>
            \Pr_X:\begin{array}[t]{rcl} \mathscr T' \amp \longrightarrow \amp [0,1] \\ A' \amp \longmapsto \amp \Pr(X\in A') \end{array}
          </me>
          est une probabilité de <m>(\Omega',\mathscr T')</m>.
        </p>
        <!--
        <p>
          Pour toute famille <m>(A_i')_{i\in I}</m> on a
          <me>
            (A_i')_{i\in I} \text{ est MI pour } \Pr_X \Longleftrightarrow \bigl((X\in A'_i)\bigr)_{i\in I} \text{ est MI pour } \Pr
          </me>
        </p>
        -->
      </statement>
    </corollary>

    <definition xml:id="def-loicouple">
      <statement>
        <p>
          Soient deux VAD <m>X</m> et <m>Y</m> définies sur <m>(\Omega,\mathscr T)</m>.
          La loi du couple <m>(X,Y)</m> est par définition la loi de la variable <m>Z=(X,Y)</m>.
          Elle est assimilée au couple <m>\Bigl(X(\Omega)\times Y(\Omega),\bigl(\Pr(X=x,Y=y)\bigr)_{x\in X(\Omega),y\in Y(\Omega)}\Bigr)</m>.
        </p>

        <p>
          En outre les lois des variables <m>X</m> et <m>Y</m> sont appelées les lois marginales du couple <m>(X,Y)</m>.
        </p>
      </statement>
    </definition>

    <remark>
      <p>
        Avec <m>Z=(X,Y)</m> on a
        <me>
          Z(\Omega)=\bigl\{(x,y)\in X(\Omega)\times Y(\Omega)\mid \exists \omega\in\Omega\;;\; x=X(\omega)\text{ et }y=Y(\omega)\bigr\}
        </me>
        on n'a donc pas nécessairement <m>Z(\Omega)=X(\Omega)\times Y(\Omega)</m> mais si <m>(x,y)\in X(\Omega)\times Y(\Omega)</m> alors
        <ul>
          <li>
            <p>
              si <m>(x,y)\in Z(\Omega)</m> alors <m>(Z=(x,y))=(X=x,Y=y)</m> et en particulier <m>\Pr\bigl(Z=(x,y)\bigr) =\Pr(X=x,Y=y)</m>;
            </p>
          </li>

          <li>
            <p>
              si <m>(x,y)\notin Z(\Omega)</m> alors <m>(Z=(x,y))=\emptyset</m> et donc <m>\Pr(X=x,Y=y)=0</m>.
            </p>
          </li>
        </ul>
        C'est pour des raisons de simplification que la loi de couple est donc définie avec <m>X(\Omega)\times Y(\Omega)</m>.
      </p>
    </remark>


    <proposition xml:id="prop-loicouple">
      <statement>
        <p>
          Soient deux VAD <m>X</m> et <m>Y</m> définies sur <m>(\Omega,\mathscr T)</m>.
          <ol>
            <li>
              <p>
                <m>\forall y\in Y(\Omega),\; \Pr(Y=y)=\sum_{x\in X(\Omega)}\Pr(X=x,Y=y)</m>
              </p>
            </li>

            <li>
              <p>
                <m>\forall x\in X(\Omega),\; \Pr(X=x)=\sum_{y\in Y(\Omega)}\Pr(X=x,Y=y)</m>
              </p>
            </li>
          </ol>
          Ce qui signifie que les lois marginales du couple <m>(X,Y)</m> sont données par sa loi de couple.
        </p>
      </statement>
    </proposition>
  </section>

  <section xml:id="sec-indvar">
    <title>Indépendance des variables aléatoires discrètes</title>

    <definition xml:id="def-varind">
      <statement>
        <p>
          Une famille <m>(X_i)_{i\in I}</m> de VAD définies sur <m>(\Omega,\mathscr T)</m> (pas nécessairement à valeurs dans le même espace) est dite <term>mutuellement indépendante</term>  (MI) si
          <me>
            \forall J\in \mathscr F(I),\; \forall (x_j)_{i\in J}\in\prod_{j\in J}X_j(\Omega),\; \Pr\Bigl(\bigcap_{j\in J}(X_j=x_j)\Bigr)=\prod_{j\in J}\Pr(X_j=x_j)
          </me>
        </p>
      </statement>
    </definition>

    <remark>
      <p>
        Si <m>(X_i)_{i\in I}</m> est une famille MI de VAD alors pour toute partie <m>I'</m> de <m>I</m> la famille <m>(X_i)_{i\in I'}</m> est MI.
      </p>
    </remark>


    <proposition xml:id="prop-varind">
      <statement>
        <p>
          Des variables aléatoires discrètes <m>X_1,X_2,\ldots,X_p</m> définies sur <m>\Omega</m> sont mutuellement indépendantes si et seulement si
          <me>
            \begin{split} \forall (x_1,x_2,\ldots,x_p)\in X_1(\Omega)\times X_2(\Omega)\times\cdots\times X_p(\Omega),\;\\ \Pr(X_1=x_1,X_2=x_2,\ldots,X_p=x_p)=\prod_{k=1}^p\Pr(X_k=x_k) \end{split}
          </me>
        </p>
      </statement>
    </proposition>

    <remark>
      <ol>
        <li>
          <p>
            Cette dernière proposition simplifie considérablement la définition de l'indépendance mutuelle d'un <em>nombre fini</em> de VAD.
          </p>
        </li>

        <li>
          <p>
            Elle implique aussi qu'une famille infinie de VAD est MI si et seulement si toutes ses sous-familles finies sont MI.
          </p>
        </li>

        <li>
          <p>
            Une suite <m>(X_n)_{n\in \N}</m> de VAD est MI si et seulement pour tout <m>n\in\N</m> les variables <m>X_0,X_1,\ldots,X_n</m> sont MI.
          </p>
        </li>
      </ol>
    </remark>


    <proposition xml:id="prop-partind">
      <statement>
        <p>
          Soit <m>(X_i)_{i\in I}</m> une famille de VAD mutuellement indépendantes et toutes définies sur <m>(\Omega,\mathscr T)</m>.
        </p>

        <p>
          Soit pour tout <m>i\in I</m>, une partie <m>A_i'</m> de <m>X_i(\Omega)</m>.
          Alors les événements <m>(X_i\in A_i'), i\in I</m> sont mutuellement indépendants.
        </p>
      </statement>
    </proposition>

    <theorem xml:id="thm-coalition">
      <statement>
        <p>
          Soit <m>(X_i)_{i\in I}</m> une famille de VAD mutuellement indépendantes et toutes définies sur <m>(\Omega,\mathscr T)</m>.
        </p>

        <ol>
          <li>
            <p>
              Si pour tout <m>i\in I</m>, <m>f_i</m> est une application définie sur <m>X_i(\Omega)</m> alors les variables <m>f_i(X_i),\; i\in I</m> sont mutuellement indépendantes.
            </p>
          </li>

          <li>
            <title>Lemme des coalitions</title>

            <p>
              Soit <m>(J_k)_{k\in K}</m> une famille de parties finies deux à deux disjointes de <m>I</m>.
              Si pour tout <m>k\in K</m>, <m>g_k</m> est une application définie sur <m>\prod_{i\in J_k}X_i(\Omega)</m> alors les variables <m>g_k\bigl((X_i)_{i\in J_k}\bigr),\;k\in K</m> sont mutuellement indépendantes.
            </p>
          </li>
        </ol>
      </statement>
    </theorem>

    <remark>
      <ol>
        <li>
          <p>
            Si <m>X</m> est une VAD presque partout constante de <m>(\Omega ,\mathscr T)</m> alors toute autre VAD définie sur <m>(\Omega ,\mathscr T)</m> est indépendante de <m>X</m>.
          </p>
          <explanation>
          <p>
            car pour tout <m>x\in X(\Omega)</m>, on a soit <m>\Pr(X=x)=0</m> soit <m>\Pr(X=x)=1</m>.
            Donc l'événement <m>(X=x)</m>  est indépendant de tou autre  événement de <m>(\Omega ,\mathscr T)</m>.
          </p>
          </explanation>
        </li>

        <li>
          <p>
            Soient <m>X</m> une VAD et <m>f</m> une fonction définie sur <m>X(\Omega)</m>.
            À moins que  <m>X</m> ou <m>f(X)</m> ne soit presque partout constante, les variables <m>X</m> et <m>f(X)</m> ne peuvent être indépendantes.
          </p>
          <explanation>
          <p>
            On suppose que <m>X</m> et <m>f(X)</m> ne sont pas presque partout constantes.
            Il existe alors <m>x_1\in X(\Omega)</m> tel que <m>0\lt \Pr(X=x_1)\lt 1</m>.
            Comme <m>(X=x_1)\subset (f(X)=f(x_1))</m>  alors <m>\Pr(f(X)=f(x_1))\gt 0</m>.
            Ensuite puisque <m>f(X)</m> est non presque partout constante alors <m>\Pr(f(X)=f(x_1))\lt 1</m> et il existe donc <m>x_2\in X(\Omega)</m> tel que <m>f(x_2)\ne f(x_1)</m> et <m>\Pr(f(X)=f(x_2))\gt0</m>.
            Ainsi
            <me>
              \bigl(X=x_1,f(X)=f(x_2)\bigr)=\emptyset \text{ et } \Pr(X=x_1)\Pr(f(X)=f(x_2))\ne0
            </me>
            <m>f</m> et <m>f(X)</m> ne sont donc pas indépendantes.
          </p>
          </explanation>
        </li>
        <!--
        <li>
          <p>
            Soient <m>X</m> et <m>Y</m> deux VAD de <m>(\Omega,\mathscr T)</m> non presque partout constantes.
            S'il existe une fonction <m>g</m> définie sur <m>X(\Omega)\times Y(\Omega)</m> telle que <m>g(X,Y)</m> soit presque partout constante alors <m>X</m> et <m>Y</m> ne sont pas indépendantes.
          </p>
          <explanation>
          <p>
            Soit <m>c\in g(X(\Omega)\times Y(\Omega))</m> tel que <m>\Pr(g(X,Y)=c)=1</m>.
            Pour tout <m>(x,y)\in X(\Omega)\times Y(\Omega)</m>, si <m>g(x,y)\ne c</m> alors <m>(X=x,Y=y)\subset (g(X,Y)\ne c)</m> et donc <m>\Pr(X=x,Y=y)=0</m>.
            On en déduit l'implication
            <me>
              g(x,y)\ne c\implies \Pr(X=x)\Pr(Y=y)=0
            </me>
          </p><
          /explanation>
        </li>
        -->
        <li>
          <title>Exemples d'utilisations du lemme des coalitions</title>

          <p>
            Soient <m>X_1,\ldots,X_p,Y</m> des VAD définies sur <m>(\Omega,\mathscr T)</m>.
            <ul>
              <li>
                <p>
                  Si <m>X_1,\ldots,X_p,Y</m> sont MI alors <m>X=(X_1,X_2,\ldots,X_p)</m> et <m>Y</m> sont MI.
                </p>
              </li>

              <li>
                <p>
                  Réciproquement si <m>X=(X_1,X_2,\ldots,X_p)</m> et <m>Y</m> sont MI alors tout ce qu'on peut dire c'est que <m>Y</m> est indépendante de <m>X_i</m> pour tout <m>i</m>.
                </p>
              </li>

              <li>
                <p>
                  Si la variable <m>Y</m> est elle même un vecteur de la forme <m>Y=(Y_1,Y_2,\ldots,Y_q)</m> et <m>X</m> et <m>Y</m> sont indépndantes alors <m>X_i</m> et <m>Y_j</m> sont indépendantes pour tous <m>i,j</m>.
                </p>
              </li>
            </ul>
          </p>
          <explanation>
          <p>
            Ce sont des conséquences du lemme des coalitions en utilisant respectivement les applications :
            <ul>
              <li>
                <p>
                  <m>g(x_1,x_2,\ldots,x_p)=(x_1,x_2,\ldots,x_p)</m>;
                </p>
              </li>

              <li>
                <p>
                  <m>g_i(x_1,x_2,\ldots,x_p)=x_i</m> ;
                </p>
              </li>

              <li>
                <p>
                  <m>g_i(x_1,x_2,\ldots,x_p)=x_i</m> et <m>h_j(y_1,y_2,\ldots,y_q)=y_j</m>
                </p>
              </li>
            </ul>
          </p>
          </explanation>
        </li>
      </ol>
    </remark>
  </section>

  <section xml:id="sec-loiusuelles">
    <title>Lois usuelles</title>

    <p>
      <m>X</m> désignera une VAD définie sur <m>(\Omega,\mathscr T)</m>
    </p>

    <ol>
      <li>
        <title>Loi de Bernouilli</title>

        <p>
          Soit un réel <m>p\in[0,1]</m> On dit que <m>X</m> suit la loi de Bernouilli de paramètre <m>p</m> et on écrit <m>X\sim \mathscr B(p)</m> si <m>X</m> est le résultat d'une expérience aléatoire qui ne possède que deux issues : succès ou échec.
          La probabilité du sucès étant <m>p</m>.
          <me>
            \begin{cases} X(\Omega)=\{1,0\} \\ \Pr(X=1)=p\text{ et } \Pr(X=0)=1-p \end{cases}
          </me>
        </p>
      </li>

      <li>
        <title>Loi binomiale</title>

        <p>
          Soit un réel <m>p\in[0,1]</m> et un entier <m>n\in\N^*</m>.
          On dit que <m>X</m> suit la loi de binomiale de paramètres <m>n</m> et <m>p</m> et on écrit <m>X\sim \mathscr B(n,p)</m> si <m>X</m> est le nombre de succès obtenus lorsque on répète <m>n</m> fois de façon indépendante une expérience de Bernouilli de paramètre <m>p</m>.
          <me>
            \begin{cases} X(\Omega)=\{0,1,\ldots,n\} \\ \forall k\in X(\Omega),\; \Pr(X=k)=\binom nk p^k(1-p)^{n-k} \end{cases}
          </me>
          <m>X</m> suit aussi la loi <m>\mathscr B(n,p)</m> si elle represente le nombre de succès obtenu lorsque on effectue simultanénement et de façon indépendante <m>n</m> test de Bernouilli de paramètre <m>p</m>.
        </p>

        <p>
          Si <m>X_k</m> est le résultat du <m>k^\text{e}</m> test de Bernouilli alors
          <md>
            <mrow> X\amp =X_1+X_2+\ldots+X_n </mrow>
            <mrow> \forall k\in[\![0,n]\!],\; (X=k)\amp=\!\!\!\!\!\bigcap_{\substack{(k_1,k_2,\ldots,k_n)\in\{0,1\}^n \\ k_1+k_2+\ldots+k_n=k}}\!\!\!\!\!(X_1=k_1,X_2=k_2,\ldots,X_n=k_n) </mrow>
          </md>
          sachant que les variables <m>X_1,X_2,\ldots,X_n</m> sont mutuellement indépendantes et suivent toute la loi <m>\mathscr B(p)</m>
        </p>
      </li>

      <li>
        <title>Loi géometrique</title>

        <p>
          Soit un réel <m>p\in]0,1[</m> On dit que <m>X</m> suit la loi géométrique de paramètre <m>p</m> et on écrit <m>X\sim \mathscr G(p)</m> si <m>X</m> est le numéro du premier test qui donne un succès lorsque on répète indéfiniment et de façon indépendante une expérience de Bernouilli de paramètre <m>p</m>.
          <me>
            \begin{cases} X(\Omega)=\N^* \\ \forall n\in\N^*,\; \Pr(X=n)=p(1-p)^{n-1} \end{cases}
          </me>
          <m>X</m> est aussi dite temps d'attente du premier succès.
        </p>

        <p>
          Si <m>X_n</m> est le résultat du <m>n^\text{e}</m> test de Bernouilli alors
          <md>
            <mrow> X \amp =\min\{n\in\N^*\mid X_n=1\} </mrow>
            <mrow> \forall n\in\N^*,\; (X=n)\amp=(X_1=0,\ldots,X_{n-1}=0,X_n=1) </mrow>
          </md>
          Sachant que les variables <m>X_n,\; n\in\N^*</m> sont mutuellement indépendantes et suivent toute la loi <m>\mathscr B(p)</m>.
        </p>
      </li>

      <li>
        <title>Loi de Poisson</title>

        <p>
          Soit un réel <m>\lambda\in\R_+</m>.
          On dit que <m>X</m> suit la loi de Poisson de paramètre <m>\lambda</m> et on écrit <m>X\sim \mathscr P(\lambda)</m> si
          <me>
            \begin{cases} X(\Omega)=\N \\ \forall n\in\N,\; \Pr(X=n)=\displaystyle\frac{\lambda^n}{n!}e^{-\lambda} \end{cases}
          </me>
          <m>X</m> représente le nombre de clients servis pendant une unité de temps dans une file d'attente quand on sait que le nombre <em>moyen</em> de clients par unité de temps est <m>\lambda</m>. Pour cette raison la loi de Poisson est aussi appelé loi des files d'attente.
        </p>
      </li>
    </ol>
  </section>

  <section xml:id="sec-loi-activite">
    <title>Activités</title>

    <activity>
      <title>Loi hypergéometrique</title>

      <statement>
        <p>
          Soient <m>p\in[0,1]</m> et <m>n,N\in\N^*</m> avec <m>n\leqslant N</m>.
          On prélève de façon équiprobable un échantillon de <m>n</m> individus dans une population de <m>N</m> individus.
          On effectue des tests de type Bernouilli sur les individus de l'échantillon sachant que la proportion d'individu positifs au test dans toute la population est <m>p</m>.
          <m>X</m> est le nombre d'individus qui s'avèrent positifs au test dans l'échantillon.
          <ol>
            <li>
              <p>
                Quelle est la loi de <m>X</m> ?
              </p>
            </li>

            <li>
              <p>
                On note <m>X_k</m> le résultat du test du <m>k^\text{e}</m> individu.
                Quelle est la loi de <m>X_k</m> ?
              </p>
            </li>
          </ol>
        </p>
      </statement>

      <answer>
        <p>
          <m>X(\Omega)\subset[\![0,n]\!]</m> et <m>\forall k\in X(\Omega),\; \displaystyle \Pr(X=k)=\frac{\binom{pN}k\binom{(1-p)N}{n-k}}{\binom Nn}</m>
        </p>
      </answer>

      <solution>
        <ol>
          <li>
            <p>
              Le nombre <m>k</m> de tests positifs dans l'échantiloon ne peut dépasser <m>n</m>, ni <m>pN</m> le nombre total d'individus positifs dans toute la population.
              D'un autre côté si <m>N-pN\lt n</m> alors on est sûr d'avoir au moins <m>n-(N-pN)</m> tests positifs dans l'échantillon.
              Ainsi
              <me>
                \max(0,n-(1-p)N)\leqslant k\leqslant \min(n,pN)
              </me>
              Ce qui suggère de prendre <m>X(\Omega)=[\![\max(0,n-(1-p)N),\min(n,pN)]\!]</m>.
              Mais pour simplifier on prend plutôt <m>X(\Omega)=[\![0,n]\!]</m> quitte à considèrer que les résultats impossibles ont une probabilité nulle.
            </p>

            <p>
              Ensuite, il y a <m>\binom Nn</m> façon de prélever équiprobablement <m>n</m> individus dans une population de <m>N</m> éléments.
              Parmi ces prélévements, ceux qui contiendront exactement <m>k</m> individus positifs sont au nombre de <m>\binom{pN}{k}\binom{N-pN}{n-k}</m> car il s'agit de prélever <m>k</m> indivdus parmi <m>pN</m> qui sont positifs au test et <m>n-k</m> individus parmi <m>N-pN</m> qui ne le sont pas.
              Vu l'équiprobabilité des prélèvements on a donc
              <me>
                \Pr(X=k)=\frac{\binom{pN}{k}\binom{(1-p)N}{n-k}}{\binom Nn}
              </me>
              On notera <m>\mathscr H(N,n,p)</m> la loi de la variable <m>X</m>.
              Elle est dite loi hypergéomètrique de paramètres <m>N,n</m> et <m>p</m>.
            </p>
          </li>

          <li>
            <p>
              Prélever un échantillon de <m>n</m> individus de façon équiprobable revient à prelever sans remise un à un et de façon équiporbable les <m>n</m> individus.
              Notons <m>X_k</m> la variable de Bernouilli qui vaut <m>1</m> si le <m>k^\text{e}</m> individu prélevé de la population est positif au test.
              Alors <m>X=X_1+X_2+\cdots+X_n</m>.
              La question précédente montre ainsi que pour tout <m>k\in[\![1,N]\!]</m>
              <me>
                S_k=X_1+X_2+\cdots+X_k\sim\mathscr H(N,k,p)
              </me>
              Soit maintenant <m>k\in[\![1,N-1]\!]</m>.
              Grâce à la formule des probabilités totales, on peut écrire
              <me>
                \Pr(X_{k+1}=1)=\sum_{i=0}^{k}\Pr(X_{k+1}=1\mid S_{k}=i)\Pr(S_{k}=i)
              </me>
              <m>\Pr(X_{k+1}=1\mid S_{k}=i)</m> est la probabilité que le <m>(k+1)^{\text{e}}</m> individu prélevé soit positif sachant que <m>i</m> individus ont été positifs pour les <m>k</m> prélévements précédents. Dans ces condition, il reste <m>N-k</m> individu dans la population dont <m>pN-i</m> sont positifs. Par équiprobabilité des prélévements on a donc
              <me>
                \Pr(X_k=1\mid S_{k-1}=i)=\frac{pN-i}{N-k}
              </me>
              Ainsi
              <md>
                <mrow> \Pr(X_{k+1}=1) \amp= \sum_{i=0}^{k}\frac{pN-i}{N-k}\cdot \frac{\binom{pN}{i}\binom{(1-p)N}{k-i}}{\binom{N}{k}} </mrow>
                <mrow> \amp=\frac{1}{(N-k)\binom Nk}\textstyle\left(pN\sum\limits_{i=0}^k\binom{pN}{i}\binom{(1-p)N}{k-i}-\sum\limits_{i=0}^k i\binom{pN}{i}\binom{(1-p)N}{k-i}\right) </mrow>
                <mrow> \amp= \frac{pN}{(N-k)\binom Nk}\left(\textstyle\sum\limits_{i=0}^k\binom{pN}{i}\binom{(1-p)N}{k-i}- \sum\limits_{i=1}^k \binom{pN-1}{i-1}\binom{(1-p)N}{k-i}\right) </mrow>
                <mrow> \amp= \frac{pN}{(N-k)\binom Nk}\left(\binom Nk-\binom{N-1}{k-1}\right) </mrow>
                <mrow> \amp= \frac{pN}{(N-k)\binom Nk}\binom{N-1}{k} </mrow>
                <mrow> \amp= \frac{pN}{N-k}\frac{(N-1)!}{k!(N-1-k)!}\frac{k!(N-k)!}{N!} </mrow>
                <mrow> \amp=p </mrow>
              </md>
              Il en ressort que malgré le changement de la répartition des cas positifs/cas négatifs après chaque prélévement, la probabulité de prélever un cas positif est toujours <m>p</m>.
            </p>
          </li>
        </ol>
      </solution>
    </activity>

    <activity>
      <title>Loi du temps d'attente du <m>k^\text{e}</m> succès </title>

      <statement>
        <p>
          Soient <m>p\in]0,1[</m> et <m>k\in N^*</m>.
          Quel est la loi du temps d'attente du <m>k^\text{e}</m> succés lorsque on répète indéfiniment et de façon indépendante une exprérience de Bernouilli de paramètre <m>p</m>
        </p>
      </statement>

      <answer>
        <p>
          <m>X(\Omega)=\{n\in\N\mid n\geqslant k\}</m> et <m>\forall n\in X(\Omega),\; \Pr(X=n)=\displaystyle\binom{n-1}{k-1}p^k(1-p)^{n-k}</m>
        </p>
      </answer>
    </activity>

    <activity>
      <title>Loi d'une marche aléatoire sur une droite</title>

      <statement>
        <p>
          Un objet se déplace sur une droite graduée.
          À chaque instant il ne peut qu'avancer d'un pas avec une probabilité <m>p</m> ou reculer d'un pas avec une probabilité <m>1-p</m>.
          Les déplacements sont tous indépendants.
        </p>

        <ol>
          <li>
            <p>
              On note <m>X_n</m> la position de l'objet sur la droite  au <m>n^\text{e}</m> pas en supposant qu'il était sur l'origine de la droite à l'instant <m>0</m>.
              Quelle est la loi de <m>X_n</m> ?
            </p>
          </li>

          <li>
            <p>
              On note <m>N</m> le numéro du premier pas pour lequel l'objet revient sur l'origine.
              Quelle est la loi de <m>N</m> ?
            </p>
          </li>
        </ol>
      </statement>

      <answer>
        <ol>
          <li>
            <p>
              <m>X_n(\Omega)=[\![-n,n]\!]</m> et <m>\Pr(X_n=k)=\displaystyle \begin{cases} 0 \amp \text{si } k+n \text{ est impaire} \\\displaystyle \binom{n}{\frac{n+k}2} \amp \text{si } k+n \text{ est paire} \end{cases} </m>
            </p>
          </li>
        </ol>
      </answer>
    </activity>

    <activity>
    <title>Comportement asymptotique d'une loi binomiale</title>
    <statement>
      <p>
      On considère une suite <m>(p_n)_{n\in\N^*}</m> de nombres réels de <m>[0,1]</m> et on suppose que <m>np_n\longrightarrow \lambda\in\R^*</m>. Soit pour tout <m>n\in\N^*</m> une variable aléatoire <m>X_n</m> qui suit la loi <m>\mathscr B(n,p_n)</m>.
    
      Déterminer pour tout entier <m>k</m> fixé, la limite de <m>\Pr(X_n=k)</m> et donner une interprétation du résultat obtenu.
      </p>
    </statement>
    <solution>
      <p>
        Fixons <m>k\in\N</m> et considérons un entier <m>n\geqslant k</m>. 
        <me>
          \Pr(X_n=k)=\binom nk p_n^k(1-p_n)^{n-k}=
          \frac1{k!}n(n-1)\cdots(n-k+1)p_n^k(1-p_n)^{n-k}
        </me>
        <m>p_n\sim\lambda/n</m> donc <m>(1-p_n)^k\longrightarrow 1</m> et donc <m>(1-p_n)^{n-k}\sim (1-p_n)^n</m>. 
        <md>
          <mrow> (1-p_n)^n \amp=
            \exp\Bigl(n\ln(1-p_n)\Bigr) 
          </mrow>
          <mrow> \amp=
          \exp\Bigl(n\ln\bigl(1-\lambda/n+o(1/n)\bigl)\Bigr)
           </mrow>
           <mrow> \amp=
           \exp\Bigl(n\bigl(-\lambda/n+o(1/n)\bigr)\Bigr)
            </mrow>
            <mrow>(1-p_n)^n \amp\longrightarrow e^{-\lambda} </mrow>
        </md>
        D'un autre côté, puisque <m>k</m> est fixé alors 
        <me>
          n(n-1)\cdots (n-k+1)p_n^k\sim (np_n)^k\longrightarrow \lambda^k
        </me>
        Ainsi 
        <m>
          \Pr(X_n=k)\longrightarrow \frac{\lambda^k}{k!}e^{-\lambda} 
        </m>, 
        ou encore 
        <me>
          \forall k\in\N^*,\; \Pr(X_n=k)\longrightarrow \Pr(X=k)
        </me>
        où <m>X</m> est une variable aléatoire qui suit la loi <m>\mathscr P(\lambda)</m>. On dit que la suite <m>(X_n)_n</m> <term>converge en loi</term>  vers <m>X</m>. 
        </p> 
        <p> Ainsi, une variable aléatoire binomiale de paramètres <m>n,p</m> se comporte lorsque <m>n</m> est grand comme une loi de Poisson de paramètre <m>\lambda\approx np</m>.
      
        
        
        
      </p>
    </solution>
    </activity>

    <activity>
      <title>Une indépendance contre-intuitive</title>

      <statement>
        <p>
          <m>N</m> suit une loi de Poisson de paramètre <m>\lambda</m>. <m>X</m> est le nombre de succès quand on répète de façon indépendante <m>N</m> test de Bernouilli de paramètre <m>p</m>.
          <ol>
            <li>
              <p>
                Déterminer la loi de <m>X</m>
              </p>
            </li>

            <li>
              <p>
                Vérifier que <m>X</m> et <m>N-X</m> sont indépendantes.
              </p>
            </li>
          </ol>
        </p>
      </statement>

      <solution>
        <ol>
          <li>
            <p>
              <m>N</m> peut potentiellement prendre toutes les valeurs dans <m>\N</m>. Il en est de même pour <m>X</m>. Ensuite pour tout <m>k\in\N</m>
              <md>
                <mrow> \Pr(X=k) \amp =\sum_{n=k}^{+\infty}\Pr(X=k\mid N=n)\Pr(N=n) </mrow>
                <mrow> \amp=\sum_{k=n}^{+\infty}\binom nk p^k(1-p)^{n-k}\cdot \frac{\lambda^n}{n!}e^{-\lambda} </mrow>
                <mrow> \amp=\lambda^ke^{-\lambda}\frac{p^k}{k!}\sum_{n=k}^{+\infty} \frac{((1-p)\lambda)^{n-k}}{(n-k)!} </mrow>
                <mrow> \amp= \frac{(p\lambda)^k}{k!}e^{-p\lambda} </mrow>
              </md>
              Ainsi <m>X\sim\mathscr P(p\lambda)</m>
            </p>
          </li>

          <li>
            <p>
              <m>N-X</m> est le nombre d'echecs pour <m>N</m> tests. Il suffit donc de remplacer <m>p</m> par <m>q=1-p</m> dans le calcul de la loi de <m>X</m> : <m>N-X\sim\mathscr P(q\lambda)</m>. Ensuite si <m>k,h\in\N</m> alors
              <md>
                <mrow>\Pr(X=k,N-X=h) \amp=\Pr(X=k,N=k+h) </mrow>
                <mrow> \amp=\Pr(X=k\mid N=k+h)\Pr(N=k+h) </mrow>
                <mrow> \amp=\binom {k+h}kp^kq^h\cdot \frac{\lambda^{k+h}}{(k+h)!}e^{-\lambda} </mrow>
                <mrow> \amp= \frac{(p\lambda)^k}{k!}e^{-p\lambda}\cdot \frac{(q\lambda)^h}{h!}e^{-q\lambda}</mrow>
                <mrow> \amp=\Pr(X=k)\Pr(N-X=h) </mrow>
              </md>
              <m>X</m> et <m>N-X</m> sont donc bien indépendantes contrairement à «l'intuition». (<m>X</m> et le nombre de succés et <m>N-X</m> le nombre d'echecs pour <m>N</m> tests.)
            </p>
          </li>
        </ol>
      </solution>
    </activity>
  </section>
</chapter>
