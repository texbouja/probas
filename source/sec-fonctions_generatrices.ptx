<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-fctgen" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Fonctions génératrices</title>

    <subsection xml:id="subsec-defprop">
        <title>Définition et propriétés</title>

        <definition xml:id="def-fctgen">
            <title>Fonction génératrice</title>

            <p>
                La fonction génératrice d'une variable aléatoire <m>X</m> à valeurs dans <m>\N</m> est définie par
                <me>             
                    G_X(t) = \mathbb E\left(t^X\right)=\sum_{n=0}^{\infty}\mathbb \Pr(X=n) t^n.
                </me>
                On notera <m>R_X</m> le rayon de convergence de la série entière <m>\sum \Pr(X=n)t^n</m>
            </p>
            </definition>

            <remark>
            <p>Soit <m>X</m> une variable aléatoire à valeurs dans <m>\N</m>.</p>
                <ol>
                    <li>
                        <p>
                            Puisque la série <m>\sum \Pr(X=n)</m> est convergente de somme <m>1</m> alors <m>R_X\geqslant 1</m> et <m>G_X(1)=1</m>.
                        </p>
                    </li>

                    <li>
                        <p>
                            La loi de <m>X</m> est entièrement déterminée par sa fonction génératrice <m>G_X</m>, dans le sens où si <m>X</m> et <m>Y</m> sont des VAD à valeurs dans <m>\N</m> alors 
                            <me>
                            X\sim Y\Longleftrightarrow G_X=G_Y
                            </me>
                            Noter aussi que dans ce cas <m>X</m> et <m>Y</m> ont les mêmes moments, en particulier la même espérance.
                         </p>
                    </li>

                    <li>
                        <p>
                            Si <m>X(\Omega)</m> est fini alors <m>G_X</m> est polynomiale de degré <m>\leqslant \max X(\Omega)</m>.
                        </p>
                    </li>

                    <li>
                        <p>
                            Si <m>R_X\gt 1</m> alors <m>X</m> admet des moments à tout ordre et pour tout <m>k\in\N^*</m>
                            <me>
                                \Es\bigl(X(X-1)\cdots(X-k+1)\bigr)=G_X^{(k)}(1)
                            </me>
                        </p>
                        <explanation>
                            <p>
                               Si <m>R_X\gt 1</m> alors <m>G_X</m> est infiniment dérivable en <m>1</m> et pour tout <m>k\in\N^*</m>
                            <me> G_X^{(k)}(1)=\sum_{n\geqslant k}n(n-1)\cdots(n-k+1)\Pr(X=n) </me>
                            La série <m>\sum n(n-1)\cdots(n-k+1)\Pr(X=n)</m> est donc convergente ce qui signifie par théorème de transfert que la variable <m>X(X-1)\cdots(X-k+1)</m> est sommable et que 
                            <me>
                                \Es\bigl(X(X-1)\cdots(X-k+1)\bigr)=G_X^{(k)}(1)
                            </me>.
                            Comme <m>X^k</m> est une combinaison linéaire de variables de la forme <m>X(X-1)\cdots(X-i+1)</m> alors <m>X^k</m> est sommable et <m>\Es(X^k)</m> s'exprime en fonction des dérivées <m>G_X^{(i)}(1)</m> pour <m>i\leqslant k</m>.
                             </p>
                        </explanation>
                            
                    </li>
                </ol>
            </remark>


            <proposition>
                <statement>
                    <p>
                        Soient <m>X</m> et <m>Y</m> deux variables aléatoires à valeurs dans <m>\N</m>.
                        Si <m>X</m> et <m>Y</m> sont indépendantes alors <m>G_{X+Y}=G_XG_Y</m>.
                    </p>
            </statement>
        </proposition>

        <corollary>
            <statement>
                <p>
                    Si <m>X_1,X_2,\ldots,X_k</m> sont des variables aléatoires mutuellement indépendantes à valeurs dans <m>\N</m> alors
                    <me>
                        G_{X_1+X_2+\cdots+X_k}=G_{X_1}G_{X_2}\cdots G_{X_k}
                    </me>
                </p>
            </statement>
        </corollary>

        <theorem>
            <statement>
                <p>
                    Soit <m>X</m> une variable aléatoire à valeurs dans <m>\N</m>.
                    <ol>
                        <li>
                            <p>
                                <m>X</m> est sommable si et seulement si <m>G_X</m> est dérivable en <m>1</m> et dans ce cas <me>\Es(X)=G_X'(1)</me>.
                            </p>
                        </li>

                        <li>
                            <p>
                                <m>X</m> est de carré sommable si et seulement si <m>G_X</m> est deux fois dérivable en <m>1</m> et dans ce cas <me>\Va(X)=G_X''(1)-G_X'(1)\left(G_X'(1)-1\right)</me>.
                            </p>
                        </li>
                    </ol>
                </p>
            </statement>
        </theorem>
    </subsection>

    <subsection xml:id="subsec-generatrices-usuelles">
        <title>Fonctions génératrices des lois usuelles</title>
        <list>
            <dl>
                <li><title>Loi uniforme</title>
                    
                    <p>
                        Si <m>X\sim \mathscr U(n)</m> à valeurs dans <m>\iic{1,n}</m> alors 
                        <m>G_X(t)=\ds\frac{1}{n}\frac{t-t^{n+1}}{1-t}</m>.
                    </p>
                </li>
                <li><title>Loi de Bernouilli</title>
                    
                    <p>
                        Si <m>X\sim\mathscr B(p)</m> alors <m>G_X(t)=1-p+pt</m>
                    </p>
                </li>
                <li><title>Loi binomiale</title>
                    
                    <p>
                        Si <m>X\sim\mathscr B(n,p)</m> alors <m>G_X(t)=(1-p+pt)^n</m>.
                    </p>
                </li>
                <li><title>Loi géométrique</title>
                    
                    <p>
                        Si <m>X\sim\mathscr G(p)</m> alors <m>G_X(t)=\ds\frac{pt}{1-(1-p)t}</m>.
                    </p>
                </li>
                <li><title>Loi de Poisson</title>
                    
                    <p>
                        Si <m>X\sim\mathscr P(\lambda)</m> alors <m>G_X(t)=e^{\lambda(t-1)}</m>.
                    </p>
                </li>
            </dl>
            
        </list>
        
    </subsection>

    <subsection xml:id="subsec-generatrice">
        <title>Concept de fonction génératrice</title>

        <exploration>
            <title>Fonction génératrice d'une suite</title>
            <p> Pour toute suite <m>(a_n)_n</m> réelle ou complexe on appelle série génératrice de <m>(a_n)_n</m>, la série entière <m>\sum a_nt^n</m>. Si le rayon de convergence de celle-ci est non nul, on appelle sa somme fonction génératrice de la suite <m>(a_n)_n</m>. 
            </p>

            <p>On appelle aussi, série génératrice exponentielle de <m>(a_n)_n</m>, la série entière <m>\sum\frac{a_n}{n!}t^n</m>. Celle-ci a plus de chance d'avoir un rayon de convergence non nul que <m>\sum a_nt^n</m>
            </p>

            <p>
                Si on connaît la fonction génératrice <m>g</m> d'une suite alors on peut retrouver la suite en calculant le développement de Taylor de <m>g</m>. D'où l'utilité des fonctions génératrices : si une suite est définie par une relation de récurrence alors on traduit, dans certains cas, cette relation en une équation fonctionnelle ou différentielle de la fonction génératrice. Résoudre cette équation nous permet ensuite d'identifier l'expression explicite des termes de la suite.  
            </p>

            

            <p>
                La fonction génératrice d'une VAD à valeurs dans <m>\N</m> est ainsi la fonction génératrice de la suite <m>(\Pr(X=n))_n</m>, ie de la loi de <m>X</m>.
            </p>
        </exploration>

        <example><title>Fonction génératrice des coefficients binomiaux</title>
            <p>
                On sait que pour tout réels <m>\alpha</m>
                <me>
                    \forall t\in]-1,1[,\; (1+t)^\alpha=\sum_{n=0}^{+\infty}\binom \alpha nt^n
                </me>
                La fonction <m>t\longmapsto (1+t)^\alpha</m> est donc la fonction génératrice des coefficients binomiaux <m>\binom \alpha n</m>. 
            </p>
            <p>
            Comme application, on peut écrire pour tous <m>\alpha, \beta\in\R</m>, <m>(1+t)^{\alpha+\beta}=(1+t)^\alpha(1+t)^\beta</m> et en identifiant les coefficients des développements en séries de Taylor des deux membres de cette égalité on obtient 
            <me>
                \forall n\in\N,\; \sum_{k=0}^n\binom \alpha  k\binom{\beta}{n-k}=\binom{\alpha+\beta}n 
            </me>
            C'est la formule de Vandermonde. 
            </p>
            <p>
                Dans le même ordre d'idée on sait que pour tout entier <m>p</m>
                <me>
                \frac{p!}{(1-t)^{p+1}}=\sum_{n=p}^{+\infty}\binom {p+n}pt^n
                </me>
                donc en identifiant les développements des deux côtés dans l'égalité suivante 
                <me>
                    \frac{1}{(1-t)^{p+q+2}}=\frac{1}{(1-t)^{p+1}}\frac{1}{(1-t)^{q+1}}
                </me>
                on obtient pour tout <m>n\in\N</m>
                <md>
                <mrow> \sum_{k=0}^n\frac{\binom{p+k}{p}}{p!}\frac{\binom{q+n-k}{q}}{q!}\amp=\frac{\binom{p+q+n+1}{p+q+1}}{(p+q+1)!} 
                </mrow>                    
                </md>
                    
                
                
                
                </p>
            </example>

            <example>
                <title>Somme de variables de Poisson MI</title>
                <p> Soient <m>X_1,X_2,\ldots,X_p</m> des variables aléatoires MI suivant des lois de Poissons de paramètre respectifs <m>
                    \lambda_1,\lambda_2,\ldots,\lambda_k</m>. On note <m>S=X_1+X_2+\cdots+X_k</m>. On veut déterminer la loi de <m>S</m>.
                </p>
                <p> On a pour tout <m>t\in\R</m> 
                    <me>
                    G_S(t)=\prod_{i=1}^kG_{X_i}(t)=\exp\bigg((t-1)\sum_{i=1}^k\lambda_i\biggr)
                    </me>
                    On en déduit que <m>S</m> suit la loi de poisson <m>\mathscr P(\lambda)</m> où <m>\lambda=\sum_{i=1}^k\lambda_i</m>.    
                    </p>
                    <p>
                        Ce résultat s'interprète par le fait que le nombre de clients servi par unité de temps dans une file d'attente qui comporte <m>k</m> guichets indépendants est une variable aléatoire suivant une loi de Poisson de paramètre <m>\lambda=\sum_{i=1}^k\lambda_i</m>.
                    </p>
                    
            </example>

            <example><title>Une formule d'inversion</title>
             <p>
                Soit <m>X</m> une VAD à valeurs dans <m>\N</m>. On suppose que <m>R_X\gt 1</m>. Alors <m>G_X</m> est développable en série de taylor en <m>1</m>, ie  
                <me>
                    G_X(t)=\sum_{n=0}^\infty \frac{G_X^{(n)}(1)}{n!}(t-1)^n=\sum_{n=0}^\infty\Es\left(\binom Xn\right)(t-1)^n
                </me>
                où <m>\binom Xn</m> désigne la variable aléatoire définie par
                <me>
                    \binom Xn=\frac{X(X-1)\cdots(X-n+1)}{n!}
                </me>
                Grâce à la convergence absolue de la série à droite dans l'égalité précédente pour <m>t</m> voisin de <m>1</m>, on peut appliquer la formule de Fubini pour transformer l'expression de <m>G_X(t)</m>
                <md>
                    <mrow>G_X(t) \amp=
                    \sum_{n=0}^{+\infty}\Es\left(\binom Xn\right)\sum_{k=0}^n(-1)^{n-k}\binom nk t^k </mrow>
                    <mrow> \amp=
                    \sum_{k=0}^{+\infty}(-1)^k\left[\sum_{n=k}^{+\infty}(-1)^{n}\binom nk\Es\left(\binom Xn\right)\right] t^k  </mrow>
                </md>
                et par identification de ce développement avec celui par défaut de <m>G_X</m> on obtient 
                <me>\forall k\in\N,\;
                    \Pr(X=k)=(-1)^k\sum_{n=k}^{+\infty}(-1)^{n}\binom nk\Es\left(\binom Xn\right)
                </me>
                Théoriquement, ces formules permettent donc de calculer la loi de <m>X</m> connaissant les moyennes de toutes les variables <m>X(X-1)\cdots(X-n+1)</m>.Par exemple 
                <md>
                    <mrow>\Pr(X=0) \amp=\sum_{n=0}^{+\infty}(-1)^{n}\Es\left(\binom Xn\right)  </mrow>
                    <mrow>\Pr(X=1) \amp =
                    \sum_{n=1}^{+\infty}(-1)^{n+1}n\Es\left(\binom Xn\right) </mrow>
                </md>
                
             </p>
             <p>
                Noter que si <m>X(\Omega)</m> est fini alors la variable <m>\binom Xn</m> et nulle pour tout <m>n</m> supérieur à <m>N=\max X(\Omega)</m> et donc que 
                <me>
                \forall k\in X(\Omega),\; \Pr(X=k)=\sum_{n=k}^{N}(-1)^{n-k}\binom nk\Es\left(\binom Xn\right)
                </me>
                qu'on peut lier aux relations obtenues par formule de transfert
                <me>
                    \forall k\in X(\Omega),\;
                    \Es\left(\binom Xk\right)=\sum_{n=k}^{N}\binom nk\Pr(X=n)
                </me>
                
              </p>
                
            </example>
        
    </subsection>

    <subsection xml:id="subsec-defprop-activite">
        <title>Activités</title>

        <activity xml:id="fonction-generatrice-xsup">
            <title>Fonction génératrice de la suite <m>(\Pr(X\geq n))_n</m></title>
            <introduction>
                <p>
                    
                    On considère une VAD <m>X</m> à valeurs dans <m>\N</m> et on pose pour tout <m>t\in]-1,1[</m>
                    <me>
                        F_X(t)=\sum_{n=0}^{+\infty}\Pr(X\geq n)t^n
                    </me>
                </p>
            </introduction>
                
                <task><title>Expression de <m>F_X</m></title>
                    <statement>
                        
                        <p> Justifier que <m>F_X</m> est bien définie et montrer que 
                        <me>
                            F_X(t)=\frac{G_X(t)-1}{t-1}
                        </me>
                        
                            
                        </p>
                    </statement>
                </task>
                <task> <title>Expression de l'espérance de <m>X</m></title>
                    
                    <statement> 
                        <p>
                            En déduire que <m>X</m> est sommable si et seulement si <m>F_X</m> admet une limite finie en <m>1</m> et que dans ce cas
                             <me>\Es(X)=\sum_{n\in\N}\Pr(X\geq n)</me>
                        </p>
                    </statement>
                </task>
                <task><title>Loi du <m>\min</m></title>
                    
                    <statement>
                        <p>
                            Soient <m>X</m> et <m>Y</m> deux VAD indépendantes  à valeurs dans <m>\N</m> et soit <m>Z=\min(X,Y)</m>. Exprimer <m>\Pr(Z\geq n)</m> en fonction de <m>\Pr(X\geq n)</m> et de <m>\Pr(Y\geq n)</m>. En déduire que pour tout <m>t\in[0,1[</m>
                            <me>
                                F_Z(t)=\frac1{2\pi}\int_0^{2\pi}F_X(t^{1/2}e^{i\theta})F_Y(t^{1/2}e^{-i\theta})\mathrm d\theta   
                            </me>
                            Simplifier cette expression dans le cas où <m>X</m> et <m>Y</m> suivent la même loi.
                            
                        </p>
                    </statement>
                    <solution>
                        <p> Pour tout <m>n\in\N</m>
                            <me>
                                \Pr(Z\geq n)=\Pr(X\geq n,Y\geq n)=\Pr(X\geq n)^2
                            </me>
                        on en déduit que 
                        <md>
                            <mrow> \Pr(Z=n)\amp=\Pr(Z\geq n)-\Pr(Z\geq n+1)=\Pr(X\geq n)^2-\Pr(X\geq n+1)^2 </mrow>
                        </md>
                        
                        
                        
                            
                            
                        </p>
                    </solution>
                </task>
            
            
        </activity>

        <activity xml:id="fonction-generatrice-premier-retour">
            <title>Fonction génératrice du premier retour à l'origine</title>
 
            <introduction>
                <p>
                    Dans cette activité, nous étudions une marche aléatoire biaisée sur une droite.
                    À chaque pas, l'objet avance d'un pas avec une probabilité <m>p</m> ou recule d'un pas avec une probabilité <m>q = 1 - p</m>.
                    Sachant qu'à l'instant <m>0</m> l'objet est à l'origine, on note <m>N</m> le temps d'attente du premier retour à l'origine et <m> X_n </m> la position de l'objet à l'instant <m> n </m>.
                </p>

                <p>
                    L'objectif est de trouver une relation entre  les fonctions génératrices
                    <me>
                        F(t) = \sum_{n=1}^{+\infty} \Pr(N = n) t^n \qtext{et} U(t) = \sum_{n=0}^{+\infty} \Pr(X_n = 0) t^n
                    </me>
                    et de calculer ensuite <m>F(t)</m>.
                </p>
            </introduction> 
            <!-- Task 1 : Définition des fonctions génératrices -->
            <task>
                <title>Définition des fonctions génératrices</title>

                <statement>
                    <p>
                        Expliquer pourquoi <m> F(t) </m> et <m> U(t) </m> sont bien définies pour <m> t \in [0, 1) </m>.
                    </p>
                </statement>

                <solution>
                    <p>
                        Les fonctions génératrices <m> F(t) </m> et <m> U(t) </m> sont les sommes de séries entières dont les coefficients sont des probabilités (donc compris entre 0 et 1).
                        Par conséquents leurs rayons de convergences sont supérieurs ou égaux à <m>1</m>.
                    </p>
                </solution>
            </task>
            <!-- Task 2 : Relation entre <m> F(t) </m> et <m> U(t) </m> -->
            <task>
                <title>Relation entre <m> F(t) </m> et <m> U(t) </m></title>

                <statement>
                    <p>
                        Montrer que les fonctions génératrices <m> F(t) </m> et <m> U(t) </m> sont liées par la relation :
                        <me>
                            U(t) = F(t) U(t) + 1.
                        </me>
                        En déduire une expression de <m> F(t) </m> en fonction de <m> U(t) </m>.
                    </p>
                </statement>

                <solution>
                    <p>
                        On peut écrire pour tout <m>n\gt0</m> :
                        <me>
                            \Pr(X_n = 0) = \sum_{k=1}^n  \Pr(X_{n} = 0\mid N=k) \Pr(N = k).
                        </me>
                        «Intuitivement» : <m>\Pr(X_n=0\mid N=k)=\Pr(X_{n-k}=0)</m> car il s'agit de revenir à l'origine en <m>n-k</m> pas une fois le retour à l'origine s'est produit pour la première fois à l'instant <m>k</m>.
                        Donc
                        <men xml:id="eq-probatota">
                            \Pr(X_n = 0) = \sum_{k=1}^n  \Pr(N=k)\Pr(X_{n-k}=0).
                        </men>
                        Plus rigoureusement, on peut écrire
                        <md>
                            <mrow>(X_n=0,N=k) \amp= (X_1X_2\cdots X_{k-1}\ne0, X_k=0, X_n=0) </mrow>
                            <mrow> \amp= (X_1X_2\cdots X_{k-1}\ne0, X_k=0, X_n-X_k=0) </mrow>
                        </md>
                        Maintenant si note <m>B_n</m> le déplacement de l'objet à l'instant <m>n</m> alors le couple <m>(X_1X_2\ldots X_{k-1},X_k)</m> est une fonction des variables <m>B_1,B_2,\ldots,B_k</m> et <m>X_n-X_k</m> est une fonction des variables <m>B_{k+1},\ldots,B_{n}</m>.
                        Donc <m>(X_1X_2\ldots X_{k-1},X_k)</m> et <m>X_n-X_k</m> sont indépendants par coalition.
                        En outre <m>X_n-X_k=B_{k+1}+\cdots+B_{n}</m> suit la même loi que <m>B_1+B_2+\cdots+B_{n-k}=X_{n-k}</m>.
                        Ainsi
                        <md>
                            <mrow> \Pr(X_n=0,N=k) \amp= \Pr(X_1X_2\cdots X_{k-1}\ne0, X_k=0)\Pr(X_n-X_k=0) </mrow>
                            <mrow>\amp= \Pr(N=k)\Pr(X_{n-k}=0) </mrow>
                        </md>
                        En multipliant <xref ref="eq-probatota"/> par <m> t^n </m> et en sommant sur <m> n </m>, on obtient :
                        <me>
                            U(t) = F(t) U(t) + 1.
                        </me>
                        En résolvant pour <m> F(t) </m>, on trouve :
                        <me>
                            F(t) = \frac{U(t) - 1}{U(t)}.
                        </me>
                    </p>
                </solution>
            </task>
            <!-- Task 3 : Calcul de <m> U(t) </m> -->
            <task>
                <title>Calcul de <m> U(t) </m></title>

                <statement>
                    <p>
                        Calculer la fonction génératrice <m> U(t) </m> en utilisant la loi de <m> X_n </m>.
                        On rappelle que <m> X_n </m> suit une loi binomiale décalée :
                        <me>
                            X_n = 2S_n - n,
                        </me>
                        où <m> S_n </m> suit une loi binomiale <m> \mathscr{B}(n, p) </m>.
                    </p>
                </statement>

                <solution>
                    <p>
                        La probabilité que <m> X_n = 0 </m> est donnée par :
                        <me>
                            \Pr(X_n = 0) = \Pr(2S_n - n = 0) = \Pr(S_n = n/2).
                        </me>
                        Comme <m> S_n </m> suit une loi binomiale <m> \mathscr{B}(n, p) </m>, on a :
                        <me>
                            \Pr(S_n = k) = \binom{n}{k} p^k q^{n - k}.
                        </me>
                        Ainsi, pour <m> n </m> pair, <m> \Pr(X_n = 0) = \binom{n}{n/2} p^{n/2} q^{n/2} </m>, et pour <m> n </m> impair, <m> \Pr(X_n = 0) = 0 </m>.
                        La fonction génératrice <m> U(t) </m> est donc :
                        <me>
                            U(t) = \sum_{n=0}^{+\infty} \binom{2n}{n} (p q)^n t^{2n}.
                        </me>
                        On pose <m>r=2\sqrt{pq}</m>, de telle sorte qu'on puisse écrire
                        <me>
                            U(t)=\sum_{n=0}^{+\infty} \frac{\binom{2n}{n}}{2^{2n}} (rt)^{2n}.
                        </me>
                        et en utilisant le développement en série entière de <m>1/\sqrt{1-t^2}</m>
                        <me>
                            U(t)=\frac{1}{\sqrt{1-(rt)^2}}=\frac{1}{\sqrt{1-4pqt^2}}.
                        </me>
                    </p>
                </solution>
            </task>
            <!-- Task 4 : Expression de <m> F(t) </m> -->
            <task>
                <title>Expression de <m> F(t) </m></title>

                <statement>
                    <p>
                        En utilisant les résultats des questions précédentes, donner une expression explicite de la fonction génératrice <m> F(t) </m>.
                    </p>
                </statement>

                <solution>
                    <p>
                        D'après la question 2, on a :
                        <me>
                            F(t) = \frac{U(t) - 1}{U(t)}.
                        </me>
                        En utilisant l'expression de <m> U(t) </m> obtenue à la question 3, on trouve :
                        <me>
                            F(t) = \frac{\frac{1}{\sqrt{1-4pqt^2}}-1}{\frac{1}{\sqrt{1-4pqt^2}}} = 1 - \sqrt{1-4pqt^2}.
                        </me>
                    </p>
                </solution>
            </task>
            <!-- Task 5 : Cas particulier <m> p = q = 1/2 </m> -->
            <task>
                <title>Calcul de la probabilité du retour vers l'origine</title>

                <statement>
                    <p>
                        Calculer la probabilité du retour vers l'origine.
                    </p>
                </statement>

                <solution>
                    <p>
                        La probabilité du retour vers l'origine est
                        <me>
                            \Pr(N\lt\infty)= \sum_{n=1}^{+\infty}\Pr(N=n)=F(1)=1-\sqrt{1-4pq}
                        </me>.
                    </p>
                </solution>
            </task>
        </activity>

        

        <activity xml:id="fonction-caracteristique-conversion">
            <title>Fonction caractéristique d'une variable aléatoire à valeurs dans <m>\Z</m></title>

            <introduction>
                <p>
                    Dans cette activité, nous étudions la fonction caractéristique d'une variable aléatoire <m> X </m> à valeurs dans <m>\Z</m>.
                    La fonction caractéristique est un outil puissant en probabilités, permettant de caractériser la loi d'une variable aléatoire et de simplifier certains calculs.
                    Nous allons définir cette fonction, explorer ses propriétés, et l'utiliser pour calculer des moments et des probabilités.
                </p>
            </introduction>
            <!-- Task 1 : Définition de la fonction caractéristique -->
            <task>
                <title>Définition de la fonction caractéristique</title>

                <statement>
                    <p>
                        Soit <m> X </m> une variable aléatoire à valeurs dans <m>\Z</m>.
                        On définit sa fonction caractéristique <m> \phi_X </m> par :
                        <me>
                            \phi_X(t) = \Es\left[e^{itX}\right] = \sum_{n\in\Z} \Pr(X = n) e^{int},
                        </me>
                        où <m> t \in \R </m>.
                    </p>

                    <p>
                        <ol>
                            <li>
                                Montrer que <m> \phi_X(t) </m> est bien définie pour tout <m> t \in \R </m>.
                            </li>

                            <li>
                                Montrer que <m> \phi_X(t) </m> est <m> 2\pi </m>-périodique et continue sur <m>\R</m>.
                            </li>
                        </ol>
                    </p>
                </statement>

                <solution>
                    <p>
                        <ol>
                            <li>
                                La fonction caractéristique <m> \phi_X(t) </m> est bien définie pour tout <m> t \in \R </m>, car <m>|e^{itX}|=1</m> et donc <m>e^{itX}</m> est sommable.
                            </li>

                            <li>
                                Les fonctions <m>t\mapsto e^{int} </m> sont <m> 2\pi </m>-périodiques, donc <m> \phi_X </m> est également <m> 2\pi </m>-périodique.
                                En outre, on peut écrire 
                                <me>
                                    \phi_X(t)=\sum_{n=0}^{+\infty}\Pr(X=n)e^{int}+\sum_{n=1}^{+\infty}\Pr(X=-n)e^{-int}.
                                </me>
                                Les sommes des deux séries dans cette expression sont continues par convergence normale sur <m>\R</m>. D'où la continuité de <m> \phi_X </m> sur <m>\R</m>.
                                
                            </li>
                        </ol>
                    </p>
                </solution>
            </task>
            <!-- Task 2 : Exemples de fonctions caractéristiques -->
            <task>
                <title>Exemples de fonctions caractéristiques</title>

                <statement>
                    <p>
                        Calculer la fonction caractéristique pour les lois suivantes :
                        <ol>
                            <li>
                                Si <m> X \sim \mathcal{B}(n, p) </m> (loi binomiale), montrer que :
                                <me>
                                    \phi_X(t) = \left(1 - p + p e^{it}\right)^n.
                                </me>
                            </li>

                            <li>
                                Si <m> X \sim \mathcal{G}(p) </m> (loi géométrique), montrer que :
                                <me>
                                    \phi_X(t) = \frac{p e^{it}}{1 - (1 - p) e^{it}}.
                                </me>
                            </li>

                            <li>
                                Si <m> X \sim \mathcal{P}(\lambda) </m> (loi de Poisson), montrer que :
                                <me>
                                    \phi_X(t) = e^{\lambda (e^{it} - 1)}.
                                </me>
                            </li>
                        </ol>
                    </p>
                </statement>

                <solution>
                    <p>
                        <ol>
                            <li>
                                Pour <m> X \sim \mathcal{B}(n, p) </m>, la fonction caractéristique est :
                                <me>
                                    \phi_X(t) = \sum_{k=0}^n \binom{n}{k} p^k (1 - p)^{n - k} e^{itk} = \left(1 - p + p e^{it}\right)^n.
                                </me>
                            </li>

                            <li>
                                Pour <m> X \sim \mathcal{G}(p) </m>, la fonction caractéristique est :
                                <me>
                                    \phi_X(t) = \sum_{k=1}^{+\infty} p (1 - p)^{k - 1} e^{itk} = \frac{p e^{it}}{1 - (1 - p) e^{it}}.
                                </me>
                            </li>

                            <li>
                                Pour <m> X \sim \mathcal{P}(\lambda) </m>, la fonction caractéristique est :
                                <me>
                                    \phi_X(t) = \sum_{k=0}^{+\infty} \frac{\lambda^k e^{-\lambda}}{k!} e^{itk} = e^{\lambda (e^{it} - 1)}.
                                </me>
                            </li>
                        </ol>
                    </p>
                </solution>
            </task>
            <!-- Task 3 : Dérivation et moments -->
            <task>
                <title>Dérivation et moments</title>

                <statement>
                    <p>
                        <ol>
                            <li>
                                Montrer que si <m> X </m> admet un moment d'ordre <m> p </m>, alors <m> \phi_X </m> est de classe <m> \mathcal{C}^p </m> et :
                                <me>
                                    \phi_X^{(k)}(0) = i^k \Es(X^k) \quad \text{pour tout } k \in \{1, \dots, p\}.
                                </me>
                            </li>

                            <li>
                                Montrer que si <m>X</m> admet des moments à tout ordre et s'il existe des constantes <m>c,r\gt 0</m> telles que 
                                <me>
                                    \forall n\in\N,\; \Es(|X|^n)\leq cr^n
                                </me>
                                alors <m>\phi_X</m> est développable en série entière sur <m>\R</m> et  
                                <me>
                                    \phi_X(t)=\sum_{n=0}^{+\infty}\frac{i^n}{n!}\Es(X^n)t^n
                                </me>
                                
                                
                            </li>
                        </ol>
                    </p>
                </statement>

                <solution>
                    <p>
                        <ol>
                            <li>
                                Si <m> X </m> admet un moment d'ordre <m> p </m>, alors la série <m> \sum_{n=-\infty}^{+\infty} n^k \Pr(X = n) </m> converge pour tout <m> k \leq p </m>.
                                Par le théorème de dérivation terme à terme, <m> \phi_X </m> est de classe <m> \mathcal{C}^p </m>, et :
                                <me>
                                    \phi_X^{(k)}(0) = i^k \sum_{n=-\infty}^{+\infty} n^k \Pr(X = n) = i^k \Es(X^k).
                                </me>
                            </li>

                            <li>
                                Si <m> X </m> admet des moments à tout ordre et que la série génératrice converge, alors par l'inégalité de Taylor-Lagrange appliquée à <m> e^{iu} </m>, on a :
                                <me>
                                    \left| e^{int} - \sum_{k=0}^p \frac{i^k n^k}{k!} t^k \right| \leq \frac{|n t|^{p+1}}{(p+1)!}.
                                </me>
                                En multipliant par <m> \Pr(X = n) </m> et en sommant sur <m> \Z </m>, on obtient :
                                <me>
                                    \left| \Es(e^{itX}) - \sum_{k=0}^p \frac{i^k}{k!} \Es(X^k) t^k \right| \leq \frac{\Es\left(|X|^{p+1}\right)}{(p+1)!} |t|^{p+1}.
                                </me>
                                Ainsi, <m> \phi_X(t) = \sum_{p=0}^{+\infty} \frac{i^p}{p!} \Es(X^p) t^p </m>.
                            </li>
                        </ol>
                    </p>
                </solution>
            </task>
        </activity>
    </subsection>
</section>
