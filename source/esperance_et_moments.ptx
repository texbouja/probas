<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch-espvaralea" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Espérance et moment d'une variable aléatoire discrète</title>

    <introduction>
        <p>
            On se donne une espace probabilisé <m>(\Omega,\mathscr T,\Pr)</m>.
        </p>
    </introduction>

    <section xml:id="sec-esparance">
        <title>Esparance d'une variable aléatoire discrète</title>

        <subsection xml:id="subsec-defprop">
            <title>Définition et propriétés</title>

            <definition xml:id="def-esperance">
                <statement>
                    <p>
                        Soit <m>X</m> une variable aléatoire discrète réelle définie sur <m>(\Omega,\mathscr T)</m>.
                    </p>

                    <p>
                        On dit que <m>X</m> est sommable, ou que <m>X</m> admet une espérance, si la famille <m>\bigl(x\Pr(X=x)\bigr)_{x\in X(\Omega)}</m> est sommable.
                        On appelle alors espérance de <m>X</m> le réel
                        <me>
                            \Es(X):=\sum_{x\in X(\Omega)}x\Pr(X=x)
                        </me>
                        On notera <m>L^1(\Omega)</m> l'ensemble des VADR sommables définies sur <m>(\Omega,\mathscr T,\Pr)</m>
                    </p>
                </statement>
            </definition>

            <remark>
                <ol>
                    <li>
                        <p>
                            Quand elle définie, <m>\Es(X)</m> est le barycentre de la famille de points pondérés <m>\bigl(x,\Pr(X=x)\bigr)_{x\in X(\Omega)}</m>.
                            Pour cette raison on l'appelle aussi valeur moyenne de <m>X</m>.
                        </p>
                    </li>

                    <li>
                        <p>
                            Dans le cas d'une VADR <m>X</m> à valeurs positives ou nulles, si <m>X</m> n'est pas sommable alors on pose par convention
                            <me>
                                \Es(X)=\sum_{x\in X(\Omega)}x\Pr(X=x)=+\infty
                            </me>
                            <m>X</m> est alors sommable si et seulement si <m>\Es(X)\lt+\infty</m>.
                        </p>
                    </li>

                    <li>
                        <p>
                            En conséquence, pour une VADR <m>X</m> quelconque
                            <me>
                                X \text{ est sommable } \Longleftrightarrow \Es(|X|)\lt +\infty
                            </me>
                        </p>
                    </li>

                    <li>
                        <p>
                            La définition a un inconvénient : la somme utilisée est indexée par un ensemble qui dépend de <m>X</m>.
                            Ce qui pose un problème lorsqu'on combine entre plusieurs variables aléatoire.
                            Comme lorsque on s'intéresse à <m>E(X+Y)</m> par exemple.
                        </p>

                        <p>
                            La formule de transfert énoncée ci-après résout ce problème.
                        </p>
                    </li>

                    <li>
                        <p>
                            La définition de <m>\Es(X)</m> dépend seulement de la loi de <m>X</m>.
                            Dans ce sens deux variables qui ont la même loi ont la même espérance.
                            Nous parlerons souvent de l'espérance d'une loi de probabilité pour désigner l'espérance des variables qui suivent cette loi.
                        </p>
                    </li>

                    <li>
                        <p>
                            La définition se limite aux variables aléatoires discrète <term>réelles</term>, mais elle est tout à fait valable sans aucune modification pour les variables aléatoires discrètes à valeurs <em>complexes</em>.
                            La notion de base utilisée étant celle de familles sommables et celle-ci a été étudiée dans le cadre des familles de nombres réels ou complexes.
                        </p>
                    </li>
                </ol>
            </remark>

            <exploration>
                <ol>
                    <li xml:id="item-vecteur">
                        <p>
                            On peut généraliser la notion d'espérance à des vecteurs aléatoires discrets en utilisant les «variables composantes» : si <m>X</m> est un vecteur aléatoire discret à valeurs dans un <m>\K</m>-ev de dimension finie <m>E</m> qui s'écrit sous la forme
                            <me>
                                \forall \omega\in\Omega,\; X(\omega)=\sum_{k=1}^dX_k(\omega)e_k
                            </me>
                            où  <m>(e_1,e_2,\ldots,e_d)</m> est une base fixée de <m>E</m>, alors, par définition, <m>X</m> est sommable si et seulement si les variables <m>X_1,X_2,\ldots,X_d</m> sont sommables et dans ce cas
                            <me>
                                \Es(X):=\sum_{k=1}^d\Es(X_k)e_k
                            </me>
                            Cela pose toutefois le problème de la dépendance de cette définition par rapport à la base <m>(e_1,e_2,\ldots,e_d)</m> utilisée.
                            (Voir remarque <xref ref="item-espvect"/>)
                        </p>
                    </li>

                    <li>
                        <p>
                            Dans le cas d'une matrice aléatoire <m>X=(X_{i,j})_{\substack{1\leqslant i\leqslant p\\1\leqslant j\leqslant q}}</m> par exemple, <m>X</m> est sommable si et seulement si <m>X_{i,j}</m> est sommable pout <m>(i,j)</m> et dans ce cas, par définition
                            <me>
                                \Es(X):=\bigl(\Es(X_{i,j})\bigr)_{\substack{1\leqslant i\leqslant p\\1\leqslant j\leqslant q}}
                            </me>
                        </p>
                    </li>

                    <li>
                        <p>
                            Attention toutefois : les familles sommables de vecteurs ne sont pas au programme.
                            Donc l'écriture
                            <me>
                                \Es(X)=\sum_{x\in X(\Omega)}x\Pr(X=x)
                            </me>
                            déborde du cadre limité par celui-ci.
                        </p>
                    </li>
                </ol>
            </exploration>


            <proposition xml:id="prop-casdenom">
                <statement>
                    <p>
                        On suppose que <m>\Omega</m> est au plus dénombrable.
                        Une VADR <m>X</m> définie sur <m>(\Omega,\mathscr T)</m> est sommable si et seulement si la famille <m>\bigl(X(\omega)\Pr(\{\omega\})\bigr)</m> est sommable et dans ce cas
                        <me>
                            \Es(X)=\sum_{\omega\in \Omega}X(\omega)\Pr(\{\omega\})
                        </me>
                    </p>
                </statement>


                <proof>
                    <p>
                        La famille d'événemnts <m>\bigl((X=x)\bigr)_{x\in X(\Omega)}</m> est une partition de <m>\Omega</m> et pour tout <m>x\in X(\Omega)</m> on a
                        <me>
                            \sum_{\omega\in(X=x)}|X(\omega)|\Pr(\{\omega\})= |x|\sum_{\omega\in(X=x)}\Pr(\{\omega\})= |x|\Pr(X=x)\lt+\infty
                        </me>
                        Donc selon le théorème de sommation par paquets
                        <me>
                            \bigl(X(\omega)\Pr(\{\omega\})\bigr)_{\omega\in\Omega} \text{ est sommable} \Longleftrightarrow \bigl(x\Pr(X=x))_{x\in X(\Omega)} \text{ est sommable}
                        </me>
                        et dans ce cas
                        <me>
                            \sum_{\omega\in \Omega}X(\omega)\Pr(\{\omega\})=\sum_{x\in X(\Omega)}x\Pr(X=x)=\Es(X)
                        </me>
                    </p>
                </proof>
            </proposition>

            <remark>
                <p>
                    Apparement ce dernier résultat résout le problème de l'ecriture de <m>\Es(X)</m> en fonction de l'ensemble <m>X(\Omega)</m> (qui dépend de <m>X</m>).
                    L'intérêt est toutefois relatif car dans la pratique on ne contrôle pas l'univers <m>\Omega</m> lui même et il n'est pas nécessairement au plus dénombrable.
                </p>
            </remark>

            <theorem xml:id="thm-transfert">
                <title>(formule de transfert)</title>

                <statement>
                    <p>
                        Soit <m>X</m> une VAD définie sur <m>(\Omega,\mathscr T)</m> (possiblement à valeurs vectorielles).
                        Soit <m>f</m> une application définie sur <m>X(\Omega)</m>.
                    </p>

                    <p>
                        La variable <m>f(X)</m> est sommable si et seulement si la famille <m>\bigl(f(x)\Pr(X=x)\bigr)_{x\in X(\Omega)}</m> est sommable et dans ce cas
                        <me>
                            \Es\bigl(f(X)\bigr)=\sum_{x\in X(\Omega)}f(x)\Pr(X=x)
                        </me>
                    </p>
                </statement>


                <proof>
                    <p>
                        On pose <m>Z=f(X)</m> et on imite la démonstration de la proposition précédente en observant cette fois que <m>\bigl(f^{-1}(\{z\})\bigr)_{z\in Z(\Omega)}</m> est une partition de <m>X(\Omega)</m>.
                        Selon le théorème de sommation par paquets, <m>\bigl(f(x)\Pr(X=x)\bigr)_{x\in X(\Omega)}</m> est sommable si et seulement si
                        <me>
                            \begin{cases} \displaystyle\forall z\in Z(\Omega),\; \Bigl(|f(x)|\Pr(X=x)\Bigr)_{x\in f^{-1}(\{z\}) }\text{ est sommable ;} \\ \displaystyle\biggl(\sum_{x\in f^{-1}(\{z\})}|f(x)|\Pr(X=x)\biggr)_{z\in Z(\Omega)} \text{ est sommable.} \end{cases}
                        </me>
                        Or pour tout <m>z\in Z(\Omega)</m>
                        <me>
                            \sum_{x\in f^{-1}(\{z\})}|f(x)|\Pr(X=x)= |z|\Pr\bigl(X\in f^{-1}(\{z\})\bigr)= |z|\Pr\bigl(f(X)=z\bigr)\lt +\infty
                        </me>
                        Ce qui ramène l'équivalence à juste la deuxième condition, elle même équivalente à la sommabilité de la variable <m>Z=f(X)</m>.
                        D'où l'équivalence énoncée dans le théorème et la validité de la formule :
                        <me>
                            \Es(f(X))=\sum_{x\in X(\Omega)}f(x)\Pr(X=x)
                        </me>
                        par sommation par paquets.
                    </p>
                </proof>
            </theorem>

            <corollary xml:id="cor-transfert">
                <statement>
                    <p>
                        Soient <m>X</m> et <m>Y</m> deux VADR.
                        Soit <m>g</m> une application définie sur <m>X(\Omega)\times Y(\Omega)</m>.
                        La variable <m>g(X,Y)</m> est sommable si et seulement si la famille <m>\bigl(g(x,y)\Pr(X=x,Y=y)\bigr)_{(x,y)\in X(\Omega)\times Y(\Omega)}</m> est sommable est dans ce cas
                        <me>
                            \Es\bigl(g(X,Y)\bigr)= \sum_{(x,y)\in X(\Omega)\times Y(\Omega)}g(x,y)\Pr(X=x,Y=y)
                        </me>
                    </p>
                </statement>


                <proof>
                    <p>
                        Il suffit d'appliquer la formule de transfert au couple <m>C=(X,Y)</m>.
                    </p>
                </proof>
            </corollary>

            <remark>
                <ol>
                    <li>
                        <p>
                            La formule de transfert permet donc de combiner les résultats de deux variables <m>X</m> et <m>Y</m> en utilisant la loi conjointe du couple <m>(X,Y)</m>.
                            Ce qui résout le problème de la dépendance de l'expression de <m>\Es(X)</m> de l'ensemble <m>X(\Omega)</m>.
                        </p>

                        <p>
                            Par exemple la variable <m>X+Y</m> est sommable si et seulement si la famille <m>\bigl((x+y)\Pr(X=x,Y=y)\bigr)_{(x,y)\in X(\Omega)\times Y(\Omega)} </m> est sommable et dans ce cas
                            <me>
                                E(X+Y)=\sum_{(x,y)\in X(\Omega)\times Y(\Omega)}(x+y)\Pr(X=x,Y=y)
                            </me>
                            Noter qu'en considérant les application <m>f_1:(x,y)\to x</m> et <m>f_2:(x,y)\in y</m>, alors selon ce même résultat, les variables <m>X</m> et <m>Y</m> sont respectivement sommable si et seulement si les familles <m>\bigl(x\Pr(X=x,Y=y)\bigr)_{(x,y)\in X(\Omega)\times Y(\Omega)} </m> et <m>\bigl(y\Pr(X=x,Y=y)\bigr)_{(x,y)\in X(\Omega)\times Y(\Omega)} </m> sont sommables et dans ce cas
                            <md>
                                <mrow>\Es(X) \amp= \sum_{(x,y)\in X(\Omega)\times Y(\Omega)}x\Pr(X=x,Y=y) </mrow>
                                <mrow>\Es(Y) \amp= \sum_{(x,y)\in X(\Omega)\times Y(\Omega)}y\Pr(X=x,Y=y) </mrow>
                            </md>
                            Expressions qui utilisent notablement le même ensemble des indices pour les deux sommes.
                        </p>
                    </li>
                </ol>
            </remark>


            <proposition xml:id="prop-proprietes">
                <statement>
                    <p>
                        Toutes les variables aléatoires introduites dans la suite sont supposées définies sur l'espace <m>(\Omega,\mathscr T)</m>.
                    </p>

                    <ol>
                        <li>
                            <title>Espérance d'une variable presque partout constante</title>

                            <p>
                                Une VADR <m>X</m> presque partout constante de valeur <m>c</m> est sommable et <m>\Es(X)=c</m>.
                            </p>
                        </li>

                        <li>
                            <title>Linéarité de l'espérance</title>

                            <p>
                                Si <m>X</m> et <m>Y</m> sont deux VADR sommables de <m>(\Omega,\mathscr T)</m> alors pour tout <m>\lambda\in \R</m>, <m>X+\lambda Y</m> est sommable et
                                <me>
                                    \Es(X+\lambda Y)=\Es(X)+\lambda \Es(Y)
                                </me>
                                Ce qui signifie que <m>L^1(\Omega)</m> est un <m>\R</m>-ev et que <m>\Es</m> est une forme linéaire de <m>L^1(\Omega)</m>.
                            </p>
                        </li>

                        <li>
                            <title>Positivité de l'espérance</title>

                            <p>
                                Si <m>X</m> est une VADR positive alors <m>\Es(X)\geqslant 0</m>.
                                De plus <m>E(X)=0</m> si et seulement si <m>X</m> est presque partout nulle sur <m>\Omega</m>.
                            </p>
                        </li>

                        <li>
                            <title>Croissance et domination</title>

                            <p>
                                Soient <m>X</m> et <m>Y</m> deux VADR.
                                <ul>
                                    <li>
                                        <p>
                                            Si <m>X</m> et <m>Y</m> sont sommables et <m>X\leqslant Y</m> alors <m>\Es(X)\leqslant \Es(Y)</m>.
                                        </p>
                                    </li>

                                    <li>
                                        <p>
                                            Si <m>|X|\leqslant Y</m> et <m>Y</m> est sommable alors <m>X</m> est sommable et <m>\bigl|\Es(X)\bigr|\leqslant \Es(Y)</m>
                                        </p>
                                    </li>
                                </ul>
                            </p>
                        </li>

                        <li>
                            <title>Effet de l'indépendance</title>

                            <p>
                                Si <m>X</m> et <m>Y</m> sont deux VADR indépendantes sommables alors <m>XY</m> est sommable et <m>\Es(XY)=\Es(X)\Es(Y)</m>.
                            </p>
                        </li>

                        <li>
                            <title>Inégalité de Cauchy-Schwarz</title>

                            <p>
                                Soient <m>X</m> et <m>Y</m> deux VADR.
                                Si <m>X^2</m> et <m>Y^2</m> sont sommables alors <m>XY</m> est sommable et on a
                                <me>
                                    \bigl|\Es(XY)\bigr|\leqslant \Es(X^2)^{1/2}\Es(Y^2)^{1/2}
                                </me>
                                avec égalité si et seulement si <m>Y</m> est presque partout nulle ou s'il existe une constante <m>\lambda</m> telle que <m>X=\lambda Y</m>  presque partout sur <m>\Omega</m> (ie <m>\Pr(X=\lambda Y)=1</m>)
                            </p>
                        </li>
                    </ol>
                </statement>
            </proposition>

            <remark>
                <ol>
                    <li>
                        <p>
                            toute VADR <m>X</m> bornée est sommable.
                            De plus
                            <me>
                                \forall m\in\R_+,\; |X|\leqslant m\Longrightarrow \bigl|\Es(X)\bigr|\leqslant m
                            </me>
                        </p>
                    </li>

                    <li>
                        <p>
                            Si <m>X,Y</m> sont des VADR sommables alors <m>\max(X,Y)</m> et <m>\min(X,Y)</m> sont sommables.
                            En outre si <m>X</m> et <m>Y</m> sont à valeurs dans <m>\N</m> alors  avec
                        </p>
                        <explanation>
                        <p>
                            La sommabilité de <m>\max(X,Y)</m> et <m>min(X,Y)</m> découle de la linéarité de l'espérance et des relation
                            <md>
                                <mrow>\max(X,Y) \amp=\frac12\bigl(X+Y+|X-Y|\bigr) </mrow>
                                <mrow>\min(X,Y) \amp=\frac12\bigl(X+Y-|X-Y|\bigr) </mrow>
                            </md>
                            Ensuite si <m>X,Y</m> sont à valeurs dans <m>\N</m> alors selon la remarque précédente .
                            <md>
                                <mrow>\Es(\min(X,Y)) \amp= \sum_{n\in\N}\Pr(\min(X,Y)\gt n) </mrow>
                                <mrow> \amp= \sum_{n\in\N}\Pr(X \gt n, Y \gt n) </mrow>
                            </md>
                        </p>
                        </explanation>
                    </li>
                </ol>
            </remark>

            <exploration>
                <ol>
                    <li xml:id="item-espvect">
                        <p>
                            Soit <m>X</m> un vecteur aléatoire sommable au sens de la remarque <xref ref="item-vecteur"/> alors <m>\Es(X)</m> ne dépend pas de la base choisie dans <m>E</m>.
                        </p>
                        <explanation>
                        <p>
                            Considérons une autre base <m>(e_1',e_2',\ldots,e_d')</m> de <m>E</m>.
                            Posons pour tout <m>i\in[\![1,d]\!]</m>, <m>e_j=a_{1,j}e_1'+a_{2,j}e_2'+\cdots+a_{d,j}e_d'</m> alors pour tout <m>\omega\in\Omega</m>
                            <md>
                                <mrow> X(\omega) \amp= \sum_{j=1}^dX_j(\omega)\sum_{i=1}^da_{i,j}e_i' </mrow>
                                <mrow> \amp= \sum_{i=1}^d\biggl(\sum_{j=1}^da_{i,j}X_j(\omega)\biggr)e_i' </mrow>
                                <mrow> \amp=\sum_{i=1}^d Y_i(\omega)e_i' </mrow>
                            </md>
                            En appliquant la même recette au vecteur <m>\Es(X):=\sum_{j=1}^d\Es(X_j)e_j'</m>  on aboutit à
                            <me>
                                \Es(X)=\sum_{i=1}^d\biggl(\sum_{j=1}^da_{i,j}\Es(X_j)\biggr)e_i'
                            </me>
                            Maintenant, puisque les variables <m>X_1,X_2,\ldots,X_d</m> sont par défintion sommabales alors les variables <m>Y_1,Y_2,\ldots,Y_d</m> sont sommables par linéarité et
                            <me>
                                \forall i\in[\![1,d]\!],\; \Es(Y_i)=\sum_{j=1}^d a_{i,j}\Es(X_j)
                            </me>
                            et ainsi  <m>\Es(X)=\sum_{i=1}^d\Es(Y_i)e_i'</m>.
                            Expression qui prouve que <m>\Es(X)</m> ne dépend pas de la base utilisée.
                        </p>
                        </explanation>
                    </li>

                    <li>
                        <p>
                            Soit <m>X</m> un vecteur aléatoire à valeurs dans un espace vectoriel de dimension finie <m>E</m>.
                            Si <m>X</m> est sommable alors pour toute application linéaire <m>u</m> définie sur <m>E</m>, la variable <m>u(X)</m> est sommable et
                            <me>
                                \Es(u(X))=u(\Es(X))
                            </me>
                        </p>
                    </li>
                </ol>
            </exploration>
        </subsection>


        <subsection xml:id="subsec-exemple-esperance">
            <title>Exemples usuels</title>

            <list>
                <title>Espérances des lois usuelles</title>

                <introduction>
                    <p>
                        <m>X</m> désignera une VADR.
                    </p>
                </introduction>
                <dl>
                <li>
                    <title>Loi uniforme</title>

                    <statement>
                        <p>
                            Si <m>X\sim \mathscr U(n)</m> alors <m> \Es(X)=\displaystyle\frac1n\sum_{k=1}^nx_k </m> où <m>x_1,x_2,\ldots,x_n</m> sont les résultats possibles de l'expérience.
                        </p>
                    </statement>
                </li>

                <li>
                    <title>Loi de Bernouilli</title>

                    <statement>
                        <p>
                            Si <m>X\sim\mathscr B(p)</m> alors <m>\Es(X)=p</m>.
                        </p>
                    </statement>
                </li>

                <li>
                    <title>Loi binomiale</title>

                    <statement>
                        <p>
                            Si <m>X\sim\mathscr B(n,p)</m> alors <m>\Es(X)=np</m>.
                        </p>
                    </statement>
                </li>

                <li>
                    <title>Loi géométrique</title>

                    <statement>
                        <p>
                            Si <m>X\sim\mathscr G(p)</m> alors <m>\Es(X)=\displaystyle\frac1p</m>.
                        </p>
                    </statement>
                </li>

                <li>
                    <title>Loi de Poisson</title>

                    <statement>
                        <p>
                            Si <m>X\sim\mathscr P(\lambda)</m> alors <m>\Es(X)=\lambda</m>.
                        </p>
                    </statement>
                </li>
                </dl>
            </list>
        </subsection>


        <subsection xml:id="subsec-activites">
            <title>Activités </title>

            <activity>
                <title>Loi hypergéométrique</title>

                <statement>
                    <p>
                        Si <m>X\sim \mathscr H(N,n,p)</m> alors <m>\Es(X)=np</m>.
                    </p>
                </statement>

                <solution>
                    <p>
                        Rappelons que <m>X\sim \mathscr H(N,n,p)</m> signifie
                        <me>
                            X(\Omega)\subset [\![0,n]\!]\text{ et } \forall k\in X(\Omega),\; P(X=k)=\frac{\binom{pN}{k}\binom{(1-p)N}{n-k}}{\binom Nn}
                        </me>
                        Donc
                        <md>
                            <mrow>\Es(X) \amp= \sum_{k=0}^{n}k\frac{\binom{pN}{k}\binom{(1-p)N}{n-k}}{\binom Nn} </mrow>
                            <mrow> \amp= \sum_{k=1}^{n}\frac{pN\binom{pN-1}{k-1}\binom{(1-p)N}{n-k}}{\binom Nn} </mrow>
                            <mrow> \amp= \frac{pN}{\binom Nn}\sum_{h=0}^{n-1}\binom{pN-1}{h}\binom{(1-p)N}{n-1-h} </mrow>
                            <mrow> \amp= \frac{pN}{\binom Nn}\binom{N-1}{n-1} </mrow>
                            <mrow> \Es(X)\amp= pn </mrow>
                        </md>
                    </p>
                </solution>
            </activity>

            <activity>
                <title>Temps d'attente du <m>k^\text{e}</m> succès </title>

                <statement>
                    <p>
                        Si <m>X_k</m> est le temps d'attente du <m>k^{\text{e}}</m> succès quand on répète indéfiniment et de façon indépendante un test de Bernouilli de paramètre <m>p</m> alors <m>\Es(X_k)=\dfrac kp</m>
                    </p>
                </statement>

                <solution>
                    <p>
                        On peut écrire <m>X_k=\sum_{i=1}^k Y_i</m> avec <m>Y_1=X_1</m> et <m>Y_i=X_i-X_{i-1}</m> si <m>i\geqslant 2</m>.
                        Les variables <m>Y_i</m> sont mutuellement indépendantes et suivent toute la loi <m>\mathscr G(p)</m>, donc
                        <me>
                            \Es(X_k)=\sum_{i=1}^k\Es(Y_i)=k\Es(X_1)=\frac kp
                        </me>
                    </p>

                    <p>
                        Par calcul direct : on a déjà calculé la loi de <m>X_k</m>
                        <me>
                            \forall n\geqslant k,\; \Pr(X=n)=\binom{n-1}{k-1}p^k(1-p)^{n-k}
                        </me>
                        donc
                        <md>
                            <mrow>\Es(X_k) \amp= \sum_{n=k}^{\infty}n\binom{n-1}{k-1}p^k(1-p)^{n-k} </mrow>
                            <mrow> \amp= \sum_{n=k}^{\infty}k\binom{n}{k}p^k(1-p)^{n-k} </mrow>
                            <mrow> \amp= k\cdot \frac{p^k}{k!}\sum_{n=k}^{\infty} n(n-1)\cdots(n-k+1)(1-p)^{n-k} </mrow>
                            <mrow> \amp= k\frac{p^k}{k!}\cdot \frac{\mathrm d^k}{\mathrm dt^k}\left.\Bigl(\frac1{1-t}\Bigr)\right|_{t=1-p} </mrow>
                            <mrow> \amp= k\frac{p^k}{k!}\cdot \left.\Bigl(\frac{k!}{(1-t)^{k+1}}\Bigr)\right|_{t=1-p} </mrow>
                            <mrow> \amp= \frac kp </mrow>
                        </md>
                    </p>
                </solution>
            </activity>

            <activity>
                <title>Espérance d'une variable à valeurs dans <m>\N</m></title>

                <statement>
                    <p>
                        Une VADR <m>X</m> à valeurs dans <m>\N</m> est sommable si et seulement si la suite <m>(P(X \gt n))_{n\in \N}</m> est sommable et dans ce cas
                        <me>
                            \Es(X)=\sum_{n\in \N}\Pr(X\gt n)
                        </me>
                    </p>
                </statement>

                <solution>
                    <p>
                        La famille <m>(\Pr(X\gt n))_{n\in\N^*}</m> est à termes réels positifs, ce qui legitime le calcul suivant, y compris pour l'interversion des sommes :
                        <md>
                            <mrow>\sum_{n\in\N}\Pr(X\gt n) \amp= \sum_{n=0}^{+\infty}\sum_{k=n+1}^{+\infty} \Pr(X=k) </mrow>
                            <mrow> \amp= \sum_{k=1}^{+\infty}\sum_{n=0}^{k-1}\Pr(X=k) </mrow>
                            <mrow> \amp= \sum_{k=1}^{+\infty} k\Pr(X=k) </mrow>
                            <mrow> \amp= \Es(X) </mrow>
                        </md>
                    </p>
                </solution>
            </activity>

            <activity>
                <title>Espérance du min </title>
               <statement>
                <p>
                    On considère deux VAD <m>X,Y</m> à valeurs dans <m>\N</m>, indépendantes et de même loi.
                    Exprimer <m>\Es(\min(X,Y))</m> en fonction des probabilités <m>\Pr(X\gt n)</m>.
                    Appliquer au cas où <m>X\sim\mathscr G(p)</m> et expliquer le résultat obtenu.
                </p>
            </statement> 

            <solution>
                <p>
                    Selon l'activité précédente
                    <md>
                        <mrow>\Es(\min(X,Y)) \amp= \sum_{n\in\N}\Pr(\min(X,Y)\gt n) </mrow>
                        <mrow> \amp= \sum_{n\in\N} \Pr(X\gt n, Y\gt n) </mrow>
                        <mrow> \amp= \sum_{n\in\N}\Pr(X\gt n)\Pr(Y\gt n) </mrow>
                        <mrow> \amp= \sum_{n\in\N} \Pr(X\gt n)^2 </mrow>
                    </md>
                    Si <m>X\sim \mathscr G(p)</m> alors pour tout <m>n\in\N</m>
                    <md>
                    <mrow> 
                        \Pr(X\gt n) \amp =
                        \sum_{k=n+1}^\infty \Pr(X=k)
                         </mrow> 
                         <mrow> \amp =  \sum_{k=n+1}^\infty p(1-p)^{k-1} </mrow>
                         <mrow> \amp = p\cdot \frac{(1-p)^n}{1-(1-p)}=(1-p)^n
                          </mrow>
                    </md> 
                    et finalement 
                    <me>
                        \Es(\min(X,Y))=\sum_{n\in\N}(1-p)^{2n}=\frac1{1-(1-p)^2}
                    </me>
                    <m>\min(X,Y)</m> est le temps d'attente d'au moins un succés quand on exécute simultanement deux instances indépendantes d'un même test de Bernouilli de paramètre <m>p</m>. Si on note respectivement <m>A</m> et <m>B</m> les événements succés  pour le premier et le deuxième test, le paramètre de notre expérience est 
                    <me>
                        q=\Pr(A\cup B)=1-\Pr(A^c\cap B^c)=1-(1-p)^2
                    </me>
                    Ce qui montre qu'en fait <m>\min(X,Y)\sim\mathscr G(1-(1-p)^2)</m>
                    
                </p>
                <p>
                    Noter que si <m>X</m> et <m>Y</m> sont indépendantes mais <m>X\sim\mathscr G(p)</m> et <m>X\sim\mathscr G(q)</m> alors le même calcul similaire aboutit à 
                    <me>
                        \min(X,Y)\sim \mathcal G\bigl(1-(1-p)(1-q)\bigr)
                    </me>
                    

                </p>
            </solution>
        </activity>

        <activity>
            <title>Une composition de lois</title>
                <statement>
                    <p>
                        <m>N</m> est une variable qui suit la loi <m>\mathscr P(\lambda)</m>. <m>(X_n)_{n\in\N^*}</m> est une suite de variables de Bernouilli  de même paramètre <m>p</m>. On suppose que <m>N</m> et toutes les variables <m>X_n</m> sont mutuellement indépendantes. Déterminer la loi de la variable <m>S=X_1+X_2+\cdots+X_N</m> et calculer son espérance. Décrire une expérience aléatoire qu'on peut modéliser avec <m>S</m>.
                    </p>
                </statement>
                <solution>
                    <p>
                        <md>
                            <mrow> 
                            \Pr(S=n) \amp= 
                        \sum_{k=n}^\infty\Pr(S=n,  N=k)
                            </mrow>
                            <mrow> \amp=
                            \sum_{k=n}^\infty\Pr(X_1+X_2+\cdots+X_k=n)\Pr(N=k)
                             </mrow>
                             <mrow> \amp=
                             \sum_{k=n}^{+\infty} \binom kn p^n(1-p)^{k-n}\cdot \frac{\lambda^k}{k!}e^{-\lambda}
                              </mrow>
                              <mrow> \amp=
                              e^{-\lambda}\frac{(\lambda p)^n}{n!}\sum_{k=n}^\infty 
                              \frac{(\lambda(1-p))^{k-n}}{(k-n)!}
                               </mrow>
                               <mrow> \amp=
                               e^{-\lambda}\frac{(\lambda p)^n}{n!}\cdot e^{\lambda(1-p)}
                                </mrow>
                                <mrow> \amp=
                                \frac{(\lambda p)^n}{n!}e^{-\lambda p}
                                 </mrow>
                        </md>
                        Donc <m>S\sim\mathscr P(\lambda p)</m> et par suite <m>\Es(S)=\lambda p</m>.
                    </p>
                </solution>
        </activity>
        </subsection>
    </section>

    <section xml:id="sec-varcovar">
        <title>Variance, covariance</title>

        <definition xml:id="def-carre-sommable">
            <statement>
                <p>
                    Une VADR <m>X</m> est dite de carré sommable si <m>X^2</m> est sommable.
                </p>

                <p>
                    On notera <m>L^2(\Omega)</m> l'ensemble des VADR de carrés sommables définies sur  <m>(\Omega,\mathscr T,P)</m>.
                </p>
            </statement>
        </definition>


        <proposition xml:id="prop-ltwo">
            <statement>
                <p>
                    <m>L^2(\Omega)</m> est un <m>\R</m>-espace vectoriel et le produit de deux VADR de carrés sommables est une variable sommable.
                </p>
            </statement>


            <proof>
                <p>
                    Découle du fait que si <m>X,Y\in L^2(\Omega)</m> alors <m>|XY|\leq \frac12(X^2+Y^2)</m> et donc <m>XY\in L^1(\Omega)</m> par domination.
                    Ensuite pour tout <m>\lambda\in \R</m>
                    <md>
                        <mrow> (X+\lambda Y)^1=X^2+\lambda^2Y^2+2\lambda XY \amp\leqslant X^2+\lambda^2Y^2+\lambda(X^2+Y^2) </mrow>
                    </md>
                    et donc, par domination, <m>X+\lambda Y\in L^2(\Omega)</m>.
                    Sachant que la variable nulle est dans <m>L^2(\Omega)</m>, ceci prouve que <m>L^2(\Omega)</m> est un sous-espace vectoriel de <m>\R^\Omega</m>.
                </p>
            </proof>
        </proposition>

        <definition xml:id="def-varcovar">
            <statement>
                <ol>
                    <li>
                        <p>
                            Soit <m>X\in L^2(\Omega)</m>.
                            La variable <m>(X-E(X))^2</m> est sommable et son espérance est appelée variance de <m>X</m>.
                            On la note <m>\Va(X)</m>
                            <me>
                                \Va(X):=\Es\bigl((X-\Es(X))^2\bigr)
                            </me>
                        </p>
                    </li>

                    <li>
                        <p>
                            Soient <m>X,Y\in L^2(\Omega)</m>.
                            La variable <m>(X-\Es(X))(Y-\Es(Y))</m> est sommable et son espérance est appelée covariance des variables <m>X</m> et <m>Y</m>.
                            On la note <m>\Cov(X,Y)</m>.
                            <me>
                                \Cov(X,Y):=\Es\bigl((X-\Es(X))(Y-\Es(Y))\bigr)
                            </me>
                        </p>
                    </li>
                </ol>
            </statement>
            <explanation>
            <p>
                <m>V(X)</m> est la moyenne «quadratique» de <m>X-\Es(X)</m>. Elle mésure la moyenne de l'écart que peut prendre <m>X</m> avec sa moyenne <m>\Es(X)</m>.
            </p>

            <p>
                <m>\Cov(X,Y)</m> mésure le degré de corélation entre <m>X</m> et <m>Y</m>, c'est à dire à quels points les résultats obtenus par <m>X</m> et par <m>Y</m> s'influencent les uns les autres. On notera par exemple que si <m>X</m> et <m>Y</m> sont indépendantes (aucune corélation) alors <m>X-\Es(X)</m> et <m>Y-\Es(Y)</m> sont indépendantes et donc
                <me>
                    \Cov(X,Y)=\Es(X-\Es(X))\Es(Y-\Es(Y))=0
                </me>
            </p>
            </explanation>
        </definition>

        <remark>
            <ol>
                <li>
                    <p>
                        Noter que si <m>X\in L^2(\Omega)</m> alors <m>\Va(X)=\Cov(X,X)</m>.
                    </p>
                </li>

                <li>
                    <p>
                    </p>
                </li>
            </ol>
        </remark>


        <proposition xml:id="prop-var-identite">
            <statement>
                <p>
                    Soient <m>X,Y\in L^2(\Omega)</m>
                    <ol>
                        <li>
                            <p>
                                <m>\Va(X)\geqslant 0</m> avec égalité si et seulement si <m>X</m> est presque partout constante.
                            </p>
                        </li>

                        <li>
                            <p>
                                <m>\Va(X)=\Es(X^2)-\Es(X)^2=\Es\bigl(X(X-1)\bigr)-\Es(X)\bigl(\Es(X)-1\bigr)</m>
                            </p>
                        </li>

                        <li>
                            <p>
                                <m>\Cov(X,Y)=\Es(XY)-\Es(X)\Es(Y)</m>
                            </p>
                        </li>

                        <li>
                            <p>
                                Si <m>X</m> et <m>Y</m> sont indépendantes alors <m>\Cov(X,Y)=0</m>.
                            </p>
                        </li>
                    </ol>
                </p>
            </statement>
        </proposition>

        <theorem xml:id="thm-varcovar">
            <statement>
                <p>
                    <m>\Cov</m> est une forme bilinéaire symétrique positive de <m>L^2(\Omega)</m>. L'inégalité de Cauchy-Schwarz pour cette forme s'écrit :
                    <me>
                        \forall X,Y\in L^2(\Omega),\; \Cov(X,Y)^2\leqslant V(X)V(Y)
                    </me>
                    avec égalité si et seulement s'il existe des réels <m>\lambda</m> et <m>\mu</m> non tous les deux nuls tels que <m>\lambda X+\mu Y</m> soit presque partout constante.
                </p>
            </statement>
        </theorem>
    </section>
</chapter>
